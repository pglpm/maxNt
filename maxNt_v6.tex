\pdfoutput=1
%% Author: PGL  Porta Mana
%% Created: 2015-05-01T20:53:34+0200
%% Last-Updated: 2019-12-07T21:23:38+0100
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newif\ifarxiv
\arxivfalse
\ifarxiv\pdfmapfile{+classico.map}\fi
\newif\ifafour
\afourfalse% true = A4, false = A5
\newif\iftypodisclaim % typographical disclaim on the side
\typodisclaimtrue
\newcommand*{\memfontfamily}{zplx}
\newcommand*{\memfontpack}{newpxtext}
\documentclass[\ifafour a4paper,12pt,\else a5paper,10pt,\fi%extrafontsizes,%
onecolumn,oneside,article,%french,italian,german,swedish,latin,
british%
]{memoir}
\newcommand*{\updated}{\today}
\newcommand*{\firstdraft}{4 November 2015}
\newcommand*{\firstpublished}{***}
\newcommand*{\propertitle}{Inferring the total activity of a large neuronal population\\ from a small sample}
\newcommand*{\pdftitle}{\propertitle}
\newcommand*{\headtitle}{Inferences about large neuronal populations}
\newcommand*{\pdfauthor}{P.G.L.  Porta Mana, Y. Roudi, V. Rostami, E. Torre}
\newcommand*{\headauthor}{Porta Mana \etal}
\newcommand*{\reporthead}{}% Report number

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Calls to packages (uncomment as needed)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{pifont}

%\usepackage{fontawesome}

\usepackage[T1]{fontenc} 
\input{glyphtounicode} \pdfgentounicode=1

\usepackage[utf8]{inputenx}

%\usepackage{newunicodechar}
% \newunicodechar{Ĕ}{\u{E}}
% \newunicodechar{ĕ}{\u{e}}
% \newunicodechar{Ĭ}{\u{I}}
% \newunicodechar{ĭ}{\u{\i}}
% \newunicodechar{Ŏ}{\u{O}}
% \newunicodechar{ŏ}{\u{o}}
% \newunicodechar{Ŭ}{\u{U}}
% \newunicodechar{ŭ}{\u{u}}
% \newunicodechar{Ā}{\=A}
% \newunicodechar{ā}{\=a}
% \newunicodechar{Ē}{\=E}
% \newunicodechar{ē}{\=e}
% \newunicodechar{Ī}{\=I}
% \newunicodechar{ī}{\={\i}}
% \newunicodechar{Ō}{\=O}
% \newunicodechar{ō}{\=o}
% \newunicodechar{Ū}{\=U}
% \newunicodechar{ū}{\=u}
% \newunicodechar{Ȳ}{\=Y}
% \newunicodechar{ȳ}{\=y}

\newcommand*{\bmmax}{0} % reduce number of bold fonts, before font packages
\newcommand*{\hmmax}{0} % reduce number of heavy fonts, before font packages

\usepackage{textcomp}

%\usepackage[normalem]{ulem}% package for underlining
% \makeatletter
% \def\ssout{\bgroup \ULdepth=-.35ex%\UL@setULdepth
%  \markoverwith{\lower\ULdepth\hbox
%    {\kern-.03em\vbox{\hrule width.2em\kern1.2\p@\hrule}\kern-.03em}}%
%  \ULon}
% \makeatother

\usepackage{amsmath}

\usepackage{mathtools}
\addtolength{\jot}{\jot} % increase spacing in multiline formulae
\setlength{\multlinegap}{0pt}

%\usepackage{empheq}% automatically calls amsmath and mathtools
%\newcommand*{\widefbox}[1]{\fbox{\hspace{1em}#1\hspace{1em}}}

%\usepackage{fancybox}

%\usepackage{framed}

% \usepackage[misc]{ifsym} % for dice
% \newcommand*{\diceone}{{\scriptsize\Cube{1}}}

\usepackage{amssymb}

\usepackage{amsxtra}

\usepackage[main=british,french,italian,german,swedish,latin,esperanto]{babel}\selectlanguage{british}
\newcommand*{\langfrench}{\foreignlanguage{french}}
\newcommand*{\langgerman}{\foreignlanguage{german}}
\newcommand*{\langitalian}{\foreignlanguage{italian}}
\newcommand*{\langswedish}{\foreignlanguage{swedish}}
\newcommand*{\langlatin}{\foreignlanguage{latin}}
\newcommand*{\langnohyph}{\foreignlanguage{nohyphenation}}

\usepackage[autostyle=false,autopunct=false,english=british]{csquotes}
\setquotestyle{british}

\usepackage{amsthm}
\newcommand*{\QED}{\textsc{q.e.d.}}
\renewcommand*{\qedsymbol}{\QED}
\theoremstyle{remark}
\newtheorem{note}{Note}
\newtheorem*{remark}{Note}
\newtheoremstyle{innote}{\parsep}{\parsep}{\footnotesize}{}{}{}{0pt}{}
\theoremstyle{innote}
\newtheorem*{innote}{}

\usepackage[shortlabels,inline]{enumitem}
\SetEnumitemKey{para}{itemindent=\parindent,leftmargin=0pt,listparindent=\parindent,parsep=0pt,itemsep=\topsep}
% \begin{asparaenum} = \begin{enumerate}[para]
% \begin{inparaenum} = \begin{enumerate*}
\setlist[enumerate,2]{label=\alph*.}
\setlist[enumerate]{label=\arabic*.,leftmargin=1.5\parindent}
\setlist[itemize]{leftmargin=1.5\parindent}
\setlist[description]{leftmargin=1.5\parindent}
% old alternative:
% \setlist[enumerate,2]{label=\alph*.}
% \setlist[enumerate]{leftmargin=\parindent}
% \setlist[itemize]{leftmargin=\parindent}
% \setlist[description]{leftmargin=\parindent}

\usepackage[babel,theoremfont,largesc]{newpxtext}

\usepackage[bigdelims,nosymbolsc%,smallerops % probably arXiv doesn't have it
]{newpxmath}
\linespread{1.083}%\useosf
%% smaller operators for old version of newpxmath
\makeatletter
\def\re@DeclareMathSymbol#1#2#3#4{%
    \let#1=\undefined
    \DeclareMathSymbol{#1}{#2}{#3}{#4}}
%\re@DeclareMathSymbol{\bigsqcupop}{\mathop}{largesymbols}{"46}
%\re@DeclareMathSymbol{\bigodotop}{\mathop}{largesymbols}{"4A}
\re@DeclareMathSymbol{\bigoplusop}{\mathop}{largesymbols}{"4C}
\re@DeclareMathSymbol{\bigotimesop}{\mathop}{largesymbols}{"4E}
\re@DeclareMathSymbol{\sumop}{\mathop}{largesymbols}{"50}
\re@DeclareMathSymbol{\prodop}{\mathop}{largesymbols}{"51}
\re@DeclareMathSymbol{\bigcupop}{\mathop}{largesymbols}{"53}
\re@DeclareMathSymbol{\bigcapop}{\mathop}{largesymbols}{"54}
%\re@DeclareMathSymbol{\biguplusop}{\mathop}{largesymbols}{"55}
\re@DeclareMathSymbol{\bigwedgeop}{\mathop}{largesymbols}{"56}
\re@DeclareMathSymbol{\bigveeop}{\mathop}{largesymbols}{"57}
%\re@DeclareMathSymbol{\bigcupdotop}{\mathop}{largesymbols}{"DF}
%\re@DeclareMathSymbol{\bigcapplusop}{\mathop}{largesymbolsPXA}{"00}
%\re@DeclareMathSymbol{\bigsqcupplusop}{\mathop}{largesymbolsPXA}{"02}
%\re@DeclareMathSymbol{\bigsqcapplusop}{\mathop}{largesymbolsPXA}{"04}
%\re@DeclareMathSymbol{\bigsqcapop}{\mathop}{largesymbolsPXA}{"06}
\re@DeclareMathSymbol{\bigtimesop}{\mathop}{largesymbolsPXA}{"10}
%\re@DeclareMathSymbol{\coprodop}{\mathop}{largesymbols}{"60}
%\re@DeclareMathSymbol{\varprod}{\mathop}{largesymbolsPXA}{16}
\makeatother
%%
%% With euler font cursive for Greek letters - the [1] means 100% scaling
\DeclareFontFamily{U}{egreek}{\skewchar\font'177}%
\DeclareFontShape{U}{egreek}{m}{n}{<-6>s*[1]eurm5 <6-8>s*[1]eurm7 <8->s*[1]eurm10}{}%
\DeclareFontShape{U}{egreek}{m}{it}{<->s*[1]eurmo10}{}%
\DeclareFontShape{U}{egreek}{b}{n}{<-6>s*[1]eurb5 <6-8>s*[1]eurb7 <8->s*[1]eurb10}{}%
\DeclareFontShape{U}{egreek}{b}{it}{<->s*[1]eurbo10}{}%
\DeclareSymbolFont{egreeki}{U}{egreek}{m}{it}%
\SetSymbolFont{egreeki}{bold}{U}{egreek}{b}{it}% from the amsfonts package
\DeclareSymbolFont{egreekr}{U}{egreek}{m}{n}%
\SetSymbolFont{egreekr}{bold}{U}{egreek}{b}{n}% from the amsfonts package
% Take also \sum, \prod, \coprod symbols from Euler fonts
\DeclareFontFamily{U}{egreekx}{\skewchar\font'177}
\DeclareFontShape{U}{egreekx}{m}{n}{%
       <-7.5>s*[0.9]euex7%
    <7.5-8.5>s*[0.9]euex8%
    <8.5-9.5>s*[0.9]euex9%
    <9.5->s*[0.9]euex10%
}{}
\DeclareSymbolFont{egreekx}{U}{egreekx}{m}{n}
\DeclareMathSymbol{\sumop}{\mathop}{egreekx}{"50}
\DeclareMathSymbol{\prodop}{\mathop}{egreekx}{"51}
\DeclareMathSymbol{\coprodop}{\mathop}{egreekx}{"60}
\makeatletter
\def\sum{\DOTSI\sumop\slimits@}
\def\prod{\DOTSI\prodop\slimits@}
\def\coprod{\DOTSI\coprodop\slimits@}
\makeatother
\input{definegreek.tex}% Greek letters not usually given in LaTeX.

%\usepackage%[scaled=0.9]%
%{classico}%  Optima as sans-serif font
\renewcommand\sfdefault{uop}
\DeclareMathAlphabet{\mathsf}  {T1}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsf}{bold}{T1}{\sfdefault}{b}{sl}
\newcommand*{\mathte}[1]{\textbf{\textit{\textsf{#1}}}}
% Upright sans-serif math alphabet
% \DeclareMathAlphabet{\mathsu}  {T1}{\sfdefault}{m}{n}
% \SetMathAlphabet{\mathsu}{bold}{T1}{\sfdefault}{b}{n}

% DejaVu Mono as typewriter text
\usepackage[scaled=0.84]{DejaVuSansMono}

\usepackage{mathdots}

\usepackage[usenames]{xcolor}
% Tol (2012) colour-blind-, print-, screen-friendly colours, alternative scheme; Munsell terminology
\definecolor{mypurpleblue}{RGB}{68,119,170}
\definecolor{myblue}{RGB}{102,204,238}
\definecolor{mygreen}{RGB}{34,136,51}
\definecolor{myyellow}{RGB}{204,187,68}
\definecolor{myred}{RGB}{238,102,119}
\definecolor{myredpurple}{RGB}{170,51,119}
\definecolor{mygrey}{RGB}{187,187,187}
% Tol (2012) colour-blind-, print-, screen-friendly colours; Munsell terminology
% \definecolor{lbpurple}{RGB}{51,34,136}
% \definecolor{lblue}{RGB}{136,204,238}
% \definecolor{lbgreen}{RGB}{68,170,153}
% \definecolor{lgreen}{RGB}{17,119,51}
% \definecolor{lgyellow}{RGB}{153,153,51}
% \definecolor{lyellow}{RGB}{221,204,119}
% \definecolor{lred}{RGB}{204,102,119}
% \definecolor{lpred}{RGB}{136,34,85}
% \definecolor{lrpurple}{RGB}{170,68,153}
\definecolor{lgrey}{RGB}{221,221,221}
%\newcommand*\mycolourbox[1]{%
%\colorbox{mygrey}{\hspace{1em}#1\hspace{1em}}}
\colorlet{shadecolor}{lgrey}

\usepackage{bm}

\usepackage{microtype}

\usepackage[backend=biber,mcite,%subentry,
citestyle=authoryear-comp,bibstyle=pglpm-authoryear,autopunct=false,sorting=ny,sortcites=false,natbib=false,maxcitenames=1,maxbibnames=8,minbibnames=8,giveninits=true,uniquename=false,uniquelist=false,maxalphanames=1,block=space,hyperref=true,defernumbers=false,useprefix=true,sortupper=false,language=british,parentracker=false]{biblatex}
\DeclareSortingScheme{ny}{\sort{\field{sortname}\field{author}\field{editor}}\sort{\field{year}}}
\iffalse\makeatletter%%% replace parenthesis with brackets
\newrobustcmd*{\parentexttrack}[1]{%
  \begingroup
  \blx@blxinit
  \blx@setsfcodes
  \blx@bibopenparen#1\blx@bibcloseparen
  \endgroup}
\AtEveryCite{%
  \let\parentext=\parentexttrack%
  \let\bibopenparen=\bibopenbracket%
  \let\bibcloseparen=\bibclosebracket}
\makeatother\fi
\DefineBibliographyExtras{british}{\def\finalandcomma{\addcomma}}
\renewcommand*{\finalnamedelim}{\addcomma\space}
\setcounter{biburlnumpenalty}{1}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{1}
\DeclareDelimFormat{multicitedelim}{\addsemicolon\space}
\DeclareDelimFormat{compcitedelim}{\addsemicolon\space}
\DeclareDelimFormat{postnotedelim}{\space}
\ifarxiv\else\addbibresource{portamanabib.bib}\fi
\renewcommand{\bibfont}{\footnotesize}
%\appto{\citesetup}{\footnotesize}% smaller font for citations
\defbibheading{bibliography}[\bibname]{\section*{#1}\addcontentsline{toc}{section}{#1}%\markboth{#1}{#1}
}
\newcommand*{\citep}{\footcites}
\newcommand*{\citey}{\parencites*}
%\renewcommand*{\cite}{\parencite}
%\newcommand*{\citeps}{\parencites}
\providecommand{\href}[2]{#2}
\providecommand{\eprint}[2]{\texttt{\href{#1}{#2}}}
\newcommand*{\amp}{\&}
% \newcommand*{\citein}[2][]{\textnormal{\textcite[#1]{#2}}%\addtocategory{extras}{#2}
% }
\newcommand*{\citein}[2][]{\textnormal{\textcite[#1]{#2}}%\addtocategory{extras}{#2}
}
\newcommand*{\citebi}[2][]{\textcite[#1]{#2}%\addtocategory{extras}{#2}
}
\newcommand*{\subtitleproc}[1]{}
\newcommand*{\chapb}{ch.}
%
% \def\arxivp{}
% \def\mparcp{}
% \def\philscip{}
% \def\biorxivp{}
% \newcommand*{\arxivsi}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.\\}
% \newcommand*{\mparcsi}{\texttt{mp\_arc} eprints available at \url{http://www.ma.utexas.edu/mp_arc/}.\\}
% \newcommand*{\philscisi}{\texttt{philsci} eprints available at \url{http://philsci-archive.pitt.edu/}.\\}
% \newcommand*{\biorxivsi}{\texttt{bioRxiv} eprints available at \url{http://biorxiv.org/}.\\}
\newcommand*{\arxiveprint}[1]{%\global\def\arxivp{\arxivsi}%\citeauthor{0arxivcite}\addtocategory{ifarchcit}{0arxivcite}%eprint
\texttt{\urlalt{https://arxiv.org/abs/#1}{arXiv:\hspace{0pt}#1}}%
%\texttt{\href{http://arxiv.org/abs/#1}{\protect\url{arXiv:#1}}}%
%\renewcommand{\arxivnote}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.}
}
\newcommand*{\mparceprint}[1]{%\global\def\mparcp{\mparcsi}%\citeauthor{0mparccite}\addtocategory{ifarchcit}{0mparccite}%eprint
\texttt{\urlalt{http://www.ma.utexas.edu/mp_arc-bin/mpa?yn=#1}{mp\_arc:\hspace{0pt}#1}}%
%\texttt{\href{http://www.ma.utexas.edu/mp_arc-bin/mpa?yn=#1}{\protect\url{mp_arc:#1}}}%
%\providecommand{\mparcnote}{\texttt{mp_arc} eprints available at \url{http://www.ma.utexas.edu/mp_arc/}.}
}
\newcommand*{\haleprint}[1]{%\global\def\arxivp{\arxivsi}%\citeauthor{0arxivcite}\addtocategory{ifarchcit}{0arxivcite}%eprint
\texttt{\urlalt{https://hal.archives-ouvertes.fr/#1}{HAL:\hspace{0pt}#1}}%
%\texttt{\href{http://arxiv.org/abs/#1}{\protect\url{arXiv:#1}}}%
%\renewcommand{\arxivnote}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.}
}
\newcommand*{\philscieprint}[1]{%\global\def\philscip{\philscisi}%\citeauthor{0philscicite}\addtocategory{ifarchcit}{0philscicite}%eprint
\texttt{\urlalt{http://philsci-archive.pitt.edu/archive/#1}{PhilSci:\hspace{0pt}#1}}%
%\texttt{\href{http://philsci-archive.pitt.edu/archive/#1}{\protect\url{PhilSci:#1}}}%
%\providecommand{\mparcnote}{\texttt{philsci} eprints available at \url{http://philsci-archive.pitt.edu/}.}
}
\newcommand*{\biorxiveprint}[1]{%\global\def\biorxivp{\biorxivsi}%\citeauthor{0arxivcite}\addtocategory{ifarchcit}{0arxivcite}%eprint
\texttt{\urlalt{https://doi.org/10.1101/#1}{bioRxiv doi:\hspace{0pt}10.1101/#1}}%
%\texttt{\href{http://arxiv.org/abs/#1}{\protect\url{arXiv:#1}}}%
%\renewcommand{\arxivnote}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.}
}
\newcommand*{\osfeprint}[1]{%
\texttt{\urlalt{https://doi.org/10.17605/osf.io/#1}{Open Science Framework doi:10.17605/osf.io/#1}}%
}

\usepackage{graphicx}

%\usepackage{wrapfig}

%\usepackage{tikz-cd}

\PassOptionsToPackage{hyphens}{url}\usepackage[hypertexnames=false]{hyperref}

\usepackage[depth=4]{bookmark}
\hypersetup{colorlinks=true,bookmarksnumbered,pdfborder={0 0 0.25},citebordercolor={0.2667 0.4667 0.6667},citecolor=mypurpleblue,linkbordercolor={0.6667 0.2 0.4667},linkcolor=myredpurple,urlbordercolor={0.1333 0.5333 0.2},urlcolor=mygreen,breaklinks=true,pdftitle={\pdftitle},pdfauthor={\pdfauthor}}
% \usepackage[vertfit=local]{breakurl}% only for arXiv
\providecommand*{\urlalt}{\href}

\usepackage[british]{datetime2}
\DTMnewdatestyle{mydate}%
{% definitions
\renewcommand*{\DTMdisplaydate}[4]{%
\number##3\ \DTMenglishmonthname{##2} ##1}%
\renewcommand*{\DTMDisplaydate}{\DTMdisplaydate}%
}
\DTMsetdatestyle{mydate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Layout. I do not know on which kind of paper the reader will print the
%%% paper on (A4? letter? one-sided? double-sided?). So I choose A5, which
%%% provides a good layout for reading on screen and save paper if printed
%%% two pages per sheet. Average length line is 66 characters and page
%%% numbers are centred.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ifafour\setstocksize{297mm}{210mm}%{*}% A4
\else\setstocksize{210mm}{5.5in}%{*}% 210x139.7
\fi
\settrimmedsize{\stockheight}{\stockwidth}{*}
\setlxvchars[\normalfont] %313.3632pt for a 66-characters line
\setxlvchars[\normalfont]
\setlength{\trimtop}{0pt}
\setlength{\trimedge}{\stockwidth}
\addtolength{\trimedge}{-\paperwidth}
% The length of the normalsize alphabet is 133.05988pt - 10 pt = 26.1408pc
% The length of the normalsize alphabet is 159.6719pt - 12pt = 30.3586pc
% Bringhurst gives 32pc as boundary optimal with 69 ch per line
% The length of the normalsize alphabet is 191.60612pt - 14pt = 35.8634pc
\ifafour\settypeblocksize{*}{32pc}{1.618} % A4
%\setulmargins{*}{*}{1.667}%gives 5/3 margins % 2 or 1.667
\else\settypeblocksize{*}{26pc}{1.618}% nearer to a 66-line newpx and preserves GR
\fi
\setulmargins{*}{*}{1}%gives equal margins
\setlrmargins{*}{*}{*}
\setheadfoot{\onelineskip}{2.5\onelineskip}
\setheaderspaces{*}{2\onelineskip}{*}
\setmarginnotes{2ex}{10mm}{0pt}
\checkandfixthelayout[nearest]
\fixpdflayout
%%% End layout
%% this fixes missing white spaces
\pdfmapline{+dummy-space <dummy-space.pfb}\pdfinterwordspaceon%

%%% Sectioning
\newcommand*{\asudedication}[1]{%
{\par\centering\textit{#1}\par}}
\newenvironment{acknowledgements}{\section*{Thanks}\addcontentsline{toc}{section}{Thanks}}{\par}
\makeatletter\renewcommand{\appendix}{\par
  \bigskip{\centering
   \interlinepenalty \@M
   \normalfont
   \printchaptertitle{\sffamily\appendixpagename}\par}
  \setcounter{section}{0}%
  \gdef\@chapapp{\appendixname}%
  \gdef\thesection{\@Alph\c@section}%
  \anappendixtrue}\makeatother
\counterwithout{section}{chapter}
\setsecnumformat{\upshape\csname the#1\endcsname\quad}
\setsecheadstyle{\large\bfseries\sffamily%
\centering}
\setsubsecheadstyle{\bfseries\sffamily%
\raggedright}
%\setbeforesecskip{-1.5ex plus 1ex minus .2ex}% plus 1ex minus .2ex}
%\setaftersecskip{1.3ex plus .2ex }% plus 1ex minus .2ex}
%\setsubsubsecheadstyle{\bfseries\sffamily\slshape\raggedright}
%\setbeforesubsecskip{1.25ex plus 1ex minus .2ex }% plus 1ex minus .2ex}
%\setaftersubsecskip{-1em}%{-0.5ex plus .2ex}% plus 1ex minus .2ex}
\setsubsecindent{0pt}%0ex plus 1ex minus .2ex}
\setparaheadstyle{\bfseries\sffamily%
\raggedright}
\setcounter{secnumdepth}{2}
\setlength{\headwidth}{\textwidth}
\newcommand{\addchap}[1]{\chapter*[#1]{#1}\addcontentsline{toc}{chapter}{#1}}
\newcommand{\addsec}[1]{\section*{#1}\addcontentsline{toc}{section}{#1}}
\newcommand{\addsubsec}[1]{\subsection*{#1}\addcontentsline{toc}{subsection}{#1}}
\newcommand{\addpara}[1]{\paragraph*{#1.}\addcontentsline{toc}{subsubsection}{#1}}
\newcommand{\addparap}[1]{\paragraph*{#1}\addcontentsline{toc}{subsubsection}{#1}}

%%% Headers, footers, pagestyle
\copypagestyle{manaart}{plain}
\makeheadrule{manaart}{\headwidth}{0.5\normalrulethickness}
\makeoddhead{manaart}{%
{\footnotesize%\sffamily%
\scshape\headauthor}}{}{{\footnotesize\sffamily%
\headtitle}}
\makeoddfoot{manaart}{}{\thepage}{}
\newcommand*\autanet{\includegraphics[height=\heightof{M}]{autanet.pdf}}
\definecolor{mygray}{gray}{0.333}
\iftypodisclaim%
\ifafour\newcommand\addprintnote{\begin{picture}(0,0)%
\put(245,149){\makebox(0,0){\rotatebox{90}{\tiny\color{mygray}\textsf{This
            document is designed for screen reading and
            two-up printing on A4 or Letter paper}}}}%
\end{picture}}% A4
\else\newcommand\addprintnote{\begin{picture}(0,0)%
\put(176,112){\makebox(0,0){\rotatebox{90}{\tiny\color{mygray}\textsf{This
            document is designed for screen reading and
            two-up printing on A4 or Letter paper}}}}%
\end{picture}}\fi%afourtrue
\makeoddfoot{plain}{}{\makebox[0pt]{\thepage}\addprintnote}{}
\else
\makeoddfoot{plain}{}{\makebox[0pt]{\thepage}}{}
\fi%typodisclaimtrue
\makeoddhead{plain}{}{}{\footnotesize\reporthead}
% \copypagestyle{manainitial}{plain}
% \makeheadrule{manainitial}{\headwidth}{0.5\normalrulethickness}
% \makeoddhead{manainitial}{%
% \footnotesize\sffamily%
% \scshape\headauthor}{}{\footnotesize\sffamily%
% \headtitle}
% \makeoddfoot{manaart}{}{\thepage}{}

\pagestyle{manaart}

\setlength{\droptitle}{-3.9\onelineskip}
\pretitle{\begin{center}\Large\sffamily%
\bfseries}
\posttitle{\bigskip\end{center}}

\makeatletter\newcommand*{\atf}{\includegraphics[%trim=1pt 1pt 0pt 0pt,
totalheight=\heightof{@}]{atblack.png}}\makeatother
\providecommand{\affiliation}[1]{\textsl{\textsf{\footnotesize #1}}}
\providecommand{\epost}[1]{\texttt{\footnotesize\textless#1\textgreater}}
\providecommand{\email}[2]{\href{mailto:#1ZZ@#2 ((remove ZZ))}{#1\protect\atf#2}}

\preauthor{\vspace{-0.5\baselineskip}\begin{center}
\normalsize\sffamily%
\lineskip  0.5em}
\postauthor{\par\end{center}}
\predate{\DTMsetdatestyle{mydate}\vspace{0.5\baselineskip}%
\begin{center}\footnotesize}
\postdate{\end{center}\vspace{-\medskipamount}}

\setfloatadjustment{figure}{\footnotesize}
\captiondelim{\quad}
\captionnamefont{\footnotesize\sffamily%
}
\captiontitlefont{\footnotesize}
\firmlists*
\midsloppy
% handling orphan/widow lines, memman.pdf
% \clubpenalty=10000
% \widowpenalty=10000
% \raggedbottom
% Downes, memman.pdf
\clubpenalty=9996
\widowpenalty=9999
\brokenpenalty=4991
\predisplaypenalty=10000
\postdisplaypenalty=1549
\displaywidowpenalty=1602
\raggedbottom
\selectlanguage{british}\frenchspacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Paper's details
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\propertitle}
\author{%
\hspace*{\stretch{1}}%
\parbox{0.5\linewidth}%\makebox[0pt][c]%
{\protect\centering P.G.L.  Porta Mana\\%
\footnotesize\epost{\email{pgl}{portamana.org}}}%
\hspace*{\stretch{1}}%
%% uncomment if additional authors present
\parbox{0.5\linewidth}%\makebox[0pt][c]%
{\protect\centering Y. Roudi\\%
\footnotesize\epost{\email{yasser.roudi}{ntnu.no}}}%
\hspace*{\stretch{1}}\\%
\parbox{0.5\linewidth}%\makebox[0pt][c]%
{\protect\centering V. Rostami\\%
\footnotesize\epost{\email{vrostami}{uni-koeln.de}}}%
\hspace*{\stretch{1}}%
\parbox{0.5\linewidth}%\makebox[0pt][c]%
{\protect\centering E. Torre\\%
\footnotesize\epost{\email{torre}{ibk.baug.ethz.ch}}}%
\hspace*{\stretch{1}}%
%\quad\href{https://orcid.org/0000-0002-6070-0784}{\protect\includegraphics[scale=0.16]{orcid_32x32.png}\textsc{orcid}:0000-0002-6070-0784}%
}

\date{Draft of \today\ (first drafted \firstdraft)}
%\date{\firstpublished; updated \updated}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Macros @@@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Common ones - uncomment as needed
%\providecommand{\nequiv}{\not\equiv}
%\providecommand{\coloneqq}{\mathrel{\mathop:}=}
%\providecommand{\eqqcolon}{=\mathrel{\mathop:}}
%\providecommand{\varprod}{\prod}
\newcommand*{\de}{\partialup}%partial diff
\newcommand*{\pu}{\piup}%constant pi
\newcommand*{\delt}{\deltaup}%Kronecker, Dirac
%\newcommand*{\eps}{\varepsilonup}%Levi-Civita, Heaviside
%\newcommand*{\riem}{\zetaup}%Riemann zeta
%\providecommand{\degree}{\textdegree}% degree
%\newcommand*{\celsius}{\textcelsius}% degree Celsius
%\newcommand*{\micro}{\textmu}% degree Celsius
\newcommand*{\I}{\mathrm{i}}%imaginary unit
\newcommand*{\e}{\mathrm{e}}%Neper
\newcommand*{\di}{\mathrm{d}}%differential
%\newcommand*{\Di}{\mathrm{D}}%capital differential
%\newcommand*{\planckc}{\hslash}
%\newcommand*{\avogn}{N_{\textrm{A}}}
%\newcommand*{\NN}{\bm{\mathrm{N}}}
%\newcommand*{\ZZ}{\bm{\mathrm{Z}}}
%\newcommand*{\QQ}{\bm{\mathrm{Q}}}
\newcommand*{\RR}{\bm{\mathrm{R}}}
%\newcommand*{\CC}{\bm{\mathrm{C}}}
%\newcommand*{\nabl}{\bm{\nabla}}%nabla
%\DeclareMathOperator{\lb}{lb}%base 2 log
%\DeclareMathOperator{\tr}{tr}%trace
%\DeclareMathOperator{\card}{card}%cardinality
%\DeclareMathOperator{\im}{Im}%im part
%\DeclareMathOperator{\re}{Re}%re part
%\DeclareMathOperator{\sgn}{sgn}%signum
%\DeclareMathOperator{\ent}{ent}%integer less or equal to
%\DeclareMathOperator{\Ord}{O}%same order as
%\DeclareMathOperator{\ord}{o}%lower order than
%\newcommand*{\incr}{\triangle}%finite increment
\newcommand*{\defd}{\coloneqq}
\newcommand*{\defs}{\eqqcolon}
%\newcommand*{\Land}{\bigwedge}
%\newcommand*{\Lor}{\bigvee}
%\newcommand*{\lland}{\DOTSB\;\land\;}
%\newcommand*{\llor}{\DOTSB\;\lor\;}
%\newcommand*{\limplies}{\mathbin{\Rightarrow}}%implies
%\newcommand*{\suchthat}{\mid}%{\mathpunct{|}}%such that (eg in sets)
%\newcommand*{\with}{\colon}%with (list of indices)
%\newcommand*{\mul}{\times}%multiplication
%\newcommand*{\inn}{\cdot}%inner product
%\newcommand*{\dotv}{\mathord{\,\cdot\,}}%variable place
%\newcommand*{\comp}{\circ}%composition of functions
%\newcommand*{\con}{\mathbin{:}}%scal prod of tensors
%\newcommand*{\equi}{\sim}%equivalent to 
\renewcommand*{\asymp}{\simeq}%equivalent to 
%\newcommand*{\corr}{\mathrel{\hat{=}}}%corresponds to
%\providecommand{\varparallel}{\ensuremath{\mathbin{/\mkern-7mu/}}}%parallel (tentative symbol)
\renewcommand*{\le}{\leqslant}%less or equal
\renewcommand*{\ge}{\geqslant}%greater or equal
\DeclarePairedDelimiter\clcl{[}{]}
%\DeclarePairedDelimiter\clop{[}{[}
%\DeclarePairedDelimiter\opcl{]}{]}
%\DeclarePairedDelimiter\opop{]}{[}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
%\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\set{\{}{\}}
%\DeclareMathOperator{\pr}{P}%probability
\newcommand*{\pf}{\mathrm{p}}%probability
\newcommand*{\p}{\mathrm{P}}%probability
\newcommand*{\E}{\mathrm{E}}
\renewcommand*{\|}{\nonscript\,\vert\nonscript\;\mathopen{}}
%\DeclarePairedDelimiterX{\cond}[2]{(}{)}{#1\nonscript\,\delimsize\vert\nonscript\;\mathopen{}#2}
\DeclarePairedDelimiterX{\condt}[2]{[}{]}{#1\nonscript\,\delimsize\vert\nonscript\;\mathopen{}#2}
%\DeclarePairedDelimiterX{\conds}[2]{\{}{\}}{#1\nonscript\,\delimsize\vert\nonscript\;\mathopen{}#2}
%\newcommand*{\+}{\lor}
%\renewcommand{\*}{\land}
\newcommand*{\sect}{\S}% Sect.~
\newcommand*{\sects}{\S\S}% Sect.~
\newcommand*{\chap}{ch.}%
\newcommand*{\chaps}{chs}%
\newcommand*{\bref}{ref.}%
\newcommand*{\brefs}{refs}%
%\newcommand*{\fn}{fn}%
\newcommand*{\eqn}{eq.}%
\newcommand*{\eqns}{eqs}%
\newcommand*{\fig}{fig.}%
\newcommand*{\figs}{figs}%
\newcommand*{\vs}{{vs}}
%\newcommand*{\etc}{{etc.}}
%\newcommand*{\ie}{{i.e.}}
%\newcommand*{\ca}{{c.}}
\newcommand*{\eg}{{e.g.}}
\newcommand*{\foll}{{ff.}}
%\newcommand*{\viz}{{viz}}
\newcommand*{\cf}{{cf.}}
%\newcommand*{\Cf}{{Cf.}}
%\newcommand*{\vd}{{v.}}
\newcommand*{\etal}{{et al.}}
%\newcommand*{\etsim}{{et sim.}}
%\newcommand*{\ibid}{{ibid.}}
%\newcommand*{\sic}{{sic}}
%\newcommand*{\id}{\mathte{I}}%id matrix
%\newcommand*{\nbd}{\nobreakdash}%
%\newcommand*{\bd}{\hspace{0pt}}%
%\def\hy{-\penalty0\hskip0pt\relax}
\newcommand*{\labelbis}[1]{\tag*{(\ref{#1})$_\text{r}$}}
%\newcommand*{\mathbox}[2][.8]{\parbox[t]{#1\columnwidth}{#2}}
%\newcommand*{\zerob}[1]{\makebox[0pt][l]{#1}}
\newcommand*{\tprod}{\mathop{\textstyle\prod}\nolimits}
\newcommand*{\tsum}{\mathop{\textstyle\sum}\nolimits}
%\newcommand*{\tint}{\begingroup\textstyle\int\endgroup\nolimits}
%\newcommand*{\tland}{\mathop{\textstyle\bigwedge}\nolimits}
%\newcommand*{\tlor}{\mathop{\textstyle\bigvee}\nolimits}
%\newcommand*{\sprod}{\mathop{\textstyle\prod}}
%\newcommand*{\ssum}{\mathop{\textstyle\sum}}
%\newcommand*{\sint}{\begingroup\textstyle\int\endgroup}
%\newcommand*{\sland}{\mathop{\textstyle\bigwedge}}
%\newcommand*{\slor}{\mathop{\textstyle\bigvee}}
\newcommand*{\T}{^\intercal}%transpose
%%\newcommand*{\QEM}%{\textnormal{$\Box$}}%{\ding{167}}
%\newcommand*{\qem}{\leavevmode\unskip\penalty9999 \hbox{}\nobreak\hfill
%\quad\hbox{\QEM}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Custom macros for this file @@@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{notecolour}{RGB}{68,170,153}
\newcommand*{\puzzle}{{\fontencoding{U}\fontfamily{fontawesometwo}\selectfont\symbol{225}}}
%\newcommand*{\puzzle}{\maltese}
\newcommand{\mynote}[1]{ {\color{notecolour}\puzzle\ #1}}
\newcommand*{\widebar}[1]{{\mkern1.5mu\skew{2}\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}}

% \newcommand{\explanation}[4][t]{%\setlength{\tabcolsep}{-1ex}
% %\smash{
% \begin{tabular}[#1]{c}#2\\[0.5\jot]\rule{1pt}{#3}\\#4\end{tabular}}%}
% \newcommand*{\ptext}[1]{\text{\small #1}}
%\DeclareMathOperator*{\argsup}{arg\,sup}
\newcommand*{\dob}{degree of belief}
\newcommand*{\dobs}{degrees of belief}

\newcommand*{\tav}{\widehat} %time average
\newcommand*{\av}{\widebar} %pop average
\newcommand*{\sav}{\widebar} %subpop average

\newcommand*{\yG}{G}

%\newcommand*{\yXv}{\varSigma}
\newcommand*{\yAv}{A}
%\newcommand*{\yxv}{s}
\newcommand*{\yav}{a}
\newcommand*{\yNg}{N_{\text{g}}}
\newcommand*{\yNng}{N_{\text{ng}}}
\newcommand*{\yNN}{\varNu}
\newcommand*{\yFF}{F}
\newcommand*{\yff}{f}
\newcommand*{\yF}{\bm{\yFF}}
\newcommand*{\yf}{\bm{\yff}}
\newcommand*{\yFFm}{\yFF^{*}}
\newcommand*{\yFm}{\yF^{*}}

%\newcommand*{\yx}{\bm{\yxv}}%subpop state
%\newcommand*{\yxs}{\sav{\yx}}%subpop av state
%\newcommand*{\yX}{\bm{\yXv}}%pop state
%\newcommand*{\yXf}{\av{\yX}}%pop av state
\newcommand*{\yaa}{\bm{\yav}}%subpop value
\newcommand*{\yaas}{\sav{\yaa\yaa}}%subpop av state
%\newcommand*{\yXXf}{\av{\yAA\yAA}}%subpop av state
\newcommand*{\ya}{\yav}%subpop av value
\newcommand*{\yAA}{\bm{\yAv}}%pop value
\newcommand*{\yA}{\yAv}%pop av value
%conditional assumptions
\newcommand*{\yH}{\varIota}
\newcommand*{\yHa}{\varIota_\textrm{p}}
\newcommand*{\yHb}{\varIota_\textrm{s}}
\newcommand*{\yHc}{\varKappa}
\newcommand*{\yHd}{\varIota_\textrm{u}}
\newcommand*{\yHp}{\varIota'}
\newcommand*{\yHi}{\varIota''}
\newcommand*{\yI}{\varIota}
\newcommand*{\yD}{D}
%Lagr multipliers
\newcommand*{\yg}{\bm{c}}
\newcommand*{\yc}{\Hat{\bm{c}}}
\newcommand*{\yll}{\bm{\lambda}}
\newcommand*{\yl}{\lambda}
\newcommand*{\yk}{z}
\newcommand*{\yK}{\zeta}
%entropies
\newcommand*{\re}{\Eta}
\newcommand*{\ybu}{\re_{\text{B}}}

\newcommand*{\yr}{\bm{r}}

\newcommand*{\ynuu}{\nu}
\newcommand*{\ynu}{\bm{\ynuu}}
\newcommand*{\yjj}{\phi}
\newcommand*{\yj}{\bm{\yjj}}

\newcommand*{\prop}[1]{`#1'}

\newcommand{\appropto}{\mathrel{\vcenter{
  \offinterlineskip\halign{\hfil$##$\cr
    \propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}
% https://tex.stackexchange.com/a/33547/97039
%\newcommand*{\fm}{f}
\newcommand*{\px}{P_{\text{me}}}
\newcommand*{\pxx}{p}
\newcommand*{\pxxx}{P}
\newcommand*{\Nd}{\textrm{N}}
\newcommand*{\ycmm}{C}
\newcommand*{\ycm}{\mathte{\ycmm}}
\newcommand*{\ycs}{\bm{\sigma}}
\newcommand*{\yL}{L}

\newcommand*{\yla}{\bm{\lambda}}
\newcommand*{\yJ}{\bm{J}}
%\newcommand*{\yd}{\bm{\varDelta}}
\newcommand*{\ydi}{\varDelta}

\newcommand*{\mee}{maximum entropy}
\newcommand*{\me}{maximum-entropy}

\newcommand*{\sh}{\mathrm{\varEta}}
\newcommand*{\nat}{\textrm{nat}}
\newcommand*{\hart}{\textrm{Hart}}

\newsubfloat{figure}
\subcaptionsize{\normalfont}
\subcaptionlabelfont{\normalfont}
\subcaptionfont{\normalfont}
%%% Custom macros end @@@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Beginning of document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\firmlists
\raggedbottom
\begin{document}
\captiondelim{\quad}\captionnamefont{\footnotesize}\captiontitlefont{\footnotesize}
\selectlanguage{british}\frenchspacing
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\abstractrunin
\abslabeldelim{}
\renewcommand*{\abstractname}{}
\setlength{\absleftindent}{0pt}
\setlength{\absrightindent}{0pt}
\setlength{\abstitleskip}{-\absparindent}
\begin{abstract}\labelsep 0pt%
***
% \noindent \mynote{Abstract must be rewritten once paper is ready}This
%   work shows how to build a maximum-entropy probabilistic model for the
%   total activity of a population of neurons, given only some activity data or
%   statistics -- for example, empirical moments -- of a \emph{subpopulation}
%   thereof. This kind of model is useful because neuronal recordings are
%   always limited to a very small sample of a population of neurons.
% %
%   The model is applied to two sets of neuronal data available in the
%   literature. In some cases it makes interesting forecasts about the larger
%   population -- for example, two low-regime modes in the frequency
%   distribution for the total activity -- that are not visible in the sample
%   data or in maximum-entropy models applied only to the sample.
% %
%   For the two datasets, the maximum-entropy probability model applied only
%   to the subpopulation is compared with the marginal probability
%   distribution obtained from the maximum-entropy model applied to the full
%   population. On a linear probability scale no large differences are
%   visible, but on a logarithmic scale the two distributions show very
%   different behaviours, especially in the tails.
  \\\noindent\emph{\footnotesize Note: Dear Reader \amp\ Peer, this
    manuscript is being peer-reviewed by you. Thank you.}
% \par%\\[\jot]
% \noindent
% {\footnotesize PACS: ***}\qquad%
% {\footnotesize MSC: ***}%
%\qquad{\footnotesize Keywords: ***}
\end{abstract}
\selectlanguage{british}\frenchspacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Epigraph
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \asudedication{\small ***}
% \vspace{\bigskipamount}
% \setlength{\epigraphwidth}{.7\columnwidth}
% %\epigraphposition{flushright}
% \epigraphtextposition{flushright}
% %\epigraphsourceposition{flushright}
% \epigraphfontsize{\footnotesize}
% \setlength{\epigraphrule}{0pt}
% %\setlength{\beforeepigraphskip}{0pt}
% %\setlength{\afterepigraphskip}{0pt}
% \epigraph{\emph{text}}{source}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% BEGINNING OF MAIN TEXT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \mynote{comment about the possibility of drawing conclusions about a brain
%   area using different sets of neurons (eg because of recording across many
%   sessions)}


\section{Introduction:\\a model for collective inferences about large neuronal
  populations}
\label{sec:intro}

What correlations are dominant in the neuronal activity of a specific brain
area? How does such activity change when external stimuli or the activity
of other areas change? Does such activity range over all its mathematically
possible values, or only within restricted bounds?

Answering this kind of questions always engages an element of uncertainty.
We cannot say \enquote{the answer is such and such}; at best we can assign
degrees of reasonable belief -- that is, probabilities -- to the possible
answers. The assessment of such distributions of belief involves
experimental data, such as recordings of neuronal activity from specific
brain areas, and pre-data knowledge and hypotheses about biological
conditions and mechanisms. When we translate such pre-data beliefs into an
initial probability distribution we often simplify them in more or less
realistic ways, to make the analysis mathematically tractable. This is why
such initial probability distributions are called \enquote{models}; and
that is how we intend this word in the present work.

The best experimental measurements of instantaneous neuronal activity use
remarkable technologies, but can still only record a very small sample of
neurons -- hundreds at most -- compared to the numbers that constitute a
functionally distinguished brain region. Many probabilistic models focus on
such samples only: they neglect that the recorded neurons are a sample from
a larger population. Such isolating assumptions sometimes escape attention,
being subtly hidden in the mathematics. Some probabilistic models try to
take unrecorded neurons into account, but they do so by describing each
neuron individually, thus becoming very complex
\citep{huang2015,battistinetal2017}. It would be useful to explore models
that operate in between, addressing the activity of the larger brain area
whence the sample comes, but \emph{collectively}, that is, without asking
about individual neuronal details. Such intermediate models would be useful
at least for preliminary investigations, to help us to decide which
hypotheses to consider or discard for more complex and costly models, or to
suggest new hypotheses.

% \mynote{[Yasser:] In the present work we aim to address the issue of
%   building probability distributions over a large population using
%   recording from subpopulations by considering the distribution of total
%   population activity as an example. [} 
In the present work we propose such an intermediate probabilistic model. It
answers this question: how much was the \emph{total} activity of a large
neuronal population, given the observation of the activity of a very small
sample thereof? This model addresses a larger brain area, avoiding the
assumption of isolation of the sample; and by focusing on the total
activity, rather than the activity of individual neurons, it remains simple
and numerically tractable.


The model can be viewed as a straightforward combination of the
maximum-entropy method and basic sampling relations from the probability
calculus, discussed in \sect~\ref{sec:model}. The maximum-entropy or
minimum-relative-entropy method \citep{jaynes1957}[much clearer
in][]{jaynes1963}{sivia1996_r2006,hobsonetal1973,jaynes1985b_r1996,grandy1980}
to build initial probability distributions has been used for different
kinds of investigations about the neuronal activity of various brain areas
and about other phenomena of importance to the neurosciences, for example
gene and protein interaction \citep[for
example][]{martignonetal1995,bohteetal2000,shlensetal2006,schneidmanetal2006,tkaciketal2006,mackeetal2009b,tkaciketal2009,roudietal2009c,barreiroetal2010,gerwinnetal2010,mackeetal2011b,ganmoretal2011,cohenetal2011,granotatedgietal2013,mackeetal2013,tkaciketal2014b,shimazakietal2015,moraetal2015,lezonetal2006,weigtetal2009}.
However, our model can also be viewed as an approximation of a more
detailed model within a fully Bayesian approach; we shall constantly keep
this point of view in mind in our discussion.

We illustrate possible uses of our model in \sect~\ref{sec:application}, by
applying it to two concrete data sets:
\begin{enumerate*}[label=(\alph*)]
\item the activity of 65 neurons recorded for 20 min from a rat's Medial
  Entorhinal Cortex \citep{stensolaetal2012},
\item the activity of 159 neurons recorded for 15 min from a macaque's
  Motor Cortex \citep{rostamietal2016_r2017}.
\end{enumerate*}
For each data set the model gives us the most plausible frequencies of all
levels of total activity of a much larger population, 1\,000--10\,000
neurons in size, during the recording. For example it can tell us that 60
out of 10\,000 neurons were most likely active during 1\% of the recording
time (though not necessarily always the same 60), 250 neurons out of
10\,000 were active during 0.4\% of the recording time, and so on. The
precise meaning of this frequency distribution is explained in
\sect~\ref{sec:model}. For the two example data sets, the guessed frequency
distributions for the larger population are distinctly different from those
for the sample. For example, for the first data set the frequency
distribution for the larger population has two very distinct modes, both at
low activities %($0.7\%$ and $2.6\%$ of simultaneously active neurons
% within $3\;\mathrm{ms}$,
(see \fig~\ref{fig:plots_5mom}), whereas the frequency distribution for the
sample is monotonically decreasing with its maximum at zero activity. These
results show that the proposed model can be used for the formulation or
preliminary assessment of interesting hypotheses. We illustrate this use
with toy examples in \sects~\ref{sec:application}
and~\ref{sec:marginalization}. Note that it is not possible to guess these
features of the larger population by applying the maximum-entropy method
\emph{at the level of the sample alone}.

% We want to stress the usefulness of making a quantified guess about the
% activity of a larger brain area. Such a guess seems indeed to be the
% primary idea behind recording a sample from that area. %  Second, it allows us
% % to make comparisons across experimental sessions; such comparisons would me
% % difficult or meaningless if made with the recorded samples, which generally
% % comprise non-overlapping sets of neurons and differ in size.
% There's also an important methodological reason.

Our model also solves a methodological problem in the use of
maximum-entropy methods to assess the \enquote{cooperativity},
\enquote{interaction}, or \enquote{synchrony} in neuronal activity, for
example studying its pairwise correlations and correlations of higher order
\citep[see for
example][]{martignonetal1995,bohteetal2000,schneidmanetal2006,shlensetal2006,barreiroetal2010,ganmoretal2011,granotatedgietal2013}.
When the difference in size between a large population and a sample thereof
is too large, the sufficiency of correlations of some order for the sample
implies the \emph{lack} of sufficiency of correlations of the same order
for the larger population, and vice versa. Maximum-entropy applications at
the sample level can therefore deceive us in questions regarding the
cooperativity of the larger population. Our model addresses directly the
latter instead. We explain this point more precisely in
\sect~\ref{sec:marginalization}.


% Maximum-entropy models have often been used to test whether a smaller part
% of the sample data, for example time averages and second-order
% correlations, is sufficient for quantifying our \dob\ about some aspects of
% the neuronal activity of the sample. This use of maximum-entropy at the
% sample level, however, has an important issue. We discuss this issue and
% explain how our approach bypasses it in \sect~\ref{sec:marginalization}.
% Moreover, our proposed model gives clearer answers to questions of
% correlation sufficiency: we show this by applying it to the two data set
% above addressing an example question of sufficiency, two moments versus
% four moments.

How large is the \enquote{larger population} considered by the model
proposed here? It obviously cannot include the full brain. The size of the
larger population is determined by the validity of the formulae from
sampling theory, and can range from a set of neurons around the recording
probe to part of a brain area, depending on some assumptions about the
recording procedure and the brain region under consideration. This matter
is discussed in detail in \sect~\ref{sec:N}.

In \sect~\ref{sec:assumptions} we discuss in detail the assumptions and
approximations which define our model, from the point of view of the
probability calculus.

% \mynote{review this}We obviously don't know whether the activity levels of
% the larger population really had the frequencies given by the model, during
% the recording. The model gives our most plausible guess. In high
% dimensions, however, the features of the most plausible distribution may
% \emph{not} be typical of the majority of most plausible distributions; and
% the set of all possible frequency distributions, if the full population for
% example comprises 1\,000 neurons, is a 1\,000-dimensional space. In
% \sect~\ref{sec:assumptions} we therefore try to assess which features of
% the frequency distribution delivered by our method may be typical and
% therefore expected of the actual one. We find that general features such as
% the bimodality of the first data set are indeed typical. Maximum-entropy
% models can be considered as approximations of Bayesian models based on
% various assumptions of inferential sufficiency
% \citep{jaynes1986d_r1996,portamana2017}. How do our guesses change if we
% modify our pre-data assumptions? We show, in the same section, that the
% typical features indicated by our method are robust against such changes.

A summary of all points above and a discussion of the usefulness of the
model is given in the final \sect~\ref{sec:summary_discussion}.

\medskip


Our notation and terminology follow \textsc{iso}
\citey{iso1993,iso2006,iso2006b} standards and Jaynes
\citey{jaynes1994_r2003} for probability. We use \enquote{\dob},
\enquote{belief}, and \enquote{probability} interchangeably.




% (for a full population of 1\,000 neurons and
% a recording of 500\,000 time bins, for example, there are
% $\binom{500\,000+1\,000}{1\,000} \approx 10^{3\,131}$ possible frequency
% distributions).

% Later on we'll show that our \dobs\ can be quantified by
% exclusively using the probability calculus. This derivation will provide a
% more accurate quantification, revealing that the maximum-entropy answer is
% only a first approximation.


% \mynote{Say something more about advantages of such question/answer: for
%   example we can make statements about total activity of brain area even
%   across recordings when sampled neurons aren't the same.}

% \mynote{Add this?: from sampling theory we know that important
%   features of the full population may not be visible in a sample because
%   smoothed out. But using sampling theory in the inverse direction we can
%   infer such full-population features from the sample.}

% \mynote{To be continued after structure of the rest of the article is
%   clear. Orig intro is on p.~\pageref{orig_intro}}



\section{Model: maximum-entropy and sampling,\\ and a Bayesian perspective}
\label{sec:model}

Let's introduce some context and mathematical notation.

The context we consider is as follows. During an experimental session we
have recorded the spiking activities of $n$ neurons for a certain amount of
time. These neurons are our \enquote{sample}. Their spikes are binned into
$T$ time bins and binarized to $\set{0,1}$ values in each bin. Call
$\ya_{t}$ the number of neurons that fire during time bin $t$: this is the
\emph{total activity} of the sample, or just \enquote{activity} for short.
Obviously $\ya_{t} \in\set{0,1,\dotsc,n}$; if $\ya_{t}=0$, no neuron spikes
during bin $t$; if $\ya_{t}=n$, all spike at some point during bin $t$, and
so on. For brevity, let's say \enquote{at $t$} for \enquote{during time bin
  $t$}. If we divide the total activity by the population size we have the
\emph{normalized total activity} or population-averaged activity $\ya/n$,
ranging from $0$ to $1$ in $1/n$ steps. From the activities $\set{\ya_{t}}$
we can count how often the activity levels $\ya=0$, $\ya=1$, and so on
appeared during the recording, obtaining the distribution of measured
relative frequencies $(\yff_{\ya}) \defs \yf$. We can also consider the
(unknown) sample activity at time bins \emph{outside} of the recorded
period.


For many animal species, the neurons that are recorded within a brain area
are not specifically chosen from among the rest, owing to several limiting
factors; for example, limitations in how precisely electrodes are inserted.
The sample of recorded neurons may even change slightly across experimental
sessions that are very far apart in time. We assume that there's an area,
comprising a population of $N$ neurons, for which we believe that any other
sample of size $n$ could have equally plausibly been recorded instead of
the sample of $n$ neurons that was actually recorded. This is what we will
mean with \enquote{larger population}. This population need not be a whole
functionally or anatomically distinct region. Loosely speaking it is the
area of which we believe our sample to be \enquote{representative}
\citep[with the warnings that accompany that
term:][]{kruskaletal1979b,kruskaletal1979c}.

The total activity of the $N$ neurons at $t$ is $\yA_{t}$. The relative
frequencies of the various activity levels during the recording were
$(\yFF_{\yA}) \defs \yF$. We don't know the values $\yA_{t}$ at each $t$,
or the frequency distribution $\yF$. We only know for certain that
$\yA_{t} \in \set{0, 1, \dotsc, N}$, that $\yA_{t} \ge \ya_{t}$, and that
$N-\yA_{t} \ge n-\ya_{t}$ for obvious reasons. For the time being we assume
that we know $N$; in \sect~\ref{sec:N} we discuss the consequences of our
lack of precise knowledge about this number.

Our questions concern general features of the total activity $\yA$ of the
larger population during and after the recording, and across sessions under
the same study conditions. For example: what was its frequency distribution
$\yF$ during the recording? How much does this frequency distribution
change across sessions? How much total activity should we expect at any
time bin during a recording? The model presented here gives a probability
distribution over the answers to these questions.


\bigskip

The idea behind our approach can be easily summarized in terms of the
maximum-entropy method:
\begin{enumerate}[label=(\alph*)]
\item\label{item:sample_step} Using sampling theory we determine the
  relation between some expected values -- specifically, moments -- for the
  total activity $\ya$ of the sample and corresponding expected values %$M$
  for the total activity $\yA$ of the larger population.
\item\label{item:maxent_step} Using the maximum-entropy method we build a
  distribution $\px(\yA \| M, N)$ for the total activity of the larger
  population of $N$ neurons, using as constraints the expected values $M$
  found in the previous step.
\end{enumerate}
We now discuss these steps more in detail, but leave their precise
mathematical implementation and a more detailed list of references to
appendix~\ref{sec:derivation_maxent_fullnet}.

\medskip

Step~\ref{item:sample_step} is just an application of the probability
calculus, which gives an exact linear relation between the first $m$
moments for the larger population and the first $m$ for the sample
\citep[\eqns~(16)]{portamanaetal2015}. The ones determine the others and
vice versa at every time bin. This relation holds for any belief
distribution $\pxxx(\yA_{t})$ for the larger-population activity and its
marginal $\pxx(\ya_{t})$ for the sample activity at that bin. These two
distributions are related by
\begin{equation}
  \label{eq:conditional_hypergeometric_t}
  \pxx(\ya_{t})= \sum_{\yA_{t}=0}^{N} \yG_{\ya_{t}\,\yA_{t}} \;\pxxx(\yA_{t})
  \qquad \ya_{t} \in \set{0,\dotsc,n},
\end{equation}
where $\yG_{\ya\yA}$ is the hypergeometric distribution defined in
\eqn~\eqref{eq:conditional_hypergeometric}, characteristic of
\enquote{drawing without replacement}
\citep[\chap~3]{jaynes1994_r2003}[\sect~4.8.3]{ross1976_r2010}[\sect~II.6]{feller1950_r1968}.


This sampling relation is even more straightforward if instead of power
moments we use \emph{normalized factorial moments} \citep{potts1953}. The
$m$th normalized factorial moment of a distribution $\pxx(\ya)$ is
\begin{equation}
  \label{eq:def_factorial_moment_a}
  % \E\Biggl[\binom{\ya}{m} \Biggr]\bigg/\binom{n}{m}
  % \defd
  \sum_{\ya=0}^{n} %m!\,
  \displaystyle\binom{\ya}{m} \bigg/ \binom{n}{m}\; \pxx(\ya)
  \qquad m \in \set{1,\dotsc,n},
\end{equation}
% where $\px(\ya \| \yH)$ is the probability for activity $\ya$ given
% assumptions $\yH$; \enquote{$\ya$} may refer to any recorded bin or to a
% new bin.
that is, the expected number of distinct $m$-tuples of simultaneously
active neurons (within a time-bin's width), normalized by the maximum
possible number of distinct $m$-tuples. % For
% example, with $m=2$, if $a=4$ neurons spike in a population of $n=5$, we
% have $\tbinom{4}{2}=6$ distinct pairs of simultaneously spiking neurons,
% and the total number of distinct pairs is $\tbinom{5}{2}=10$. The
% normalized number of spiking pairs is therefore $6/10$.
Note that the first $m$ factorial moments together provide the same
information as the first $m$ power moments together, and vice versa: they
are linearly related because $\tbinom{\ya}{m}$ is a polynomial in $\ya$ of
degree $m$. So we'll just say \enquote{first $m$ moments} from now
on. % Specifying one set is therefore
% equivalent to specifying the other set.
But the normalized factorial moments have a mathematically convenient
property: \emph{the first $n$ normalized factorial moments for the sample
  and for the larger population are numerically identical}:
\begin{equation}
  \label{eq:equal_m_factorialmoments}
  % \begin{split}
  % \E\Biggl[\binom{\ya}{m} \Biggr]\bigg/\binom{n}{m}
  % &=
  % \E\Biggl[\binom{\yA}{m} \Biggr]\bigg/\binom{N}{m}
  % \qquad\text{or}\\
    \sum_{\ya=0}^{n} %m!\,
  \binom{\ya}{m}\bigg/  \binom{n}{m}\; \pxx(\ya)
=    \sum_{\yA=0}^{N} %m!\,
  \binom{\yA}{m}\bigg/  \binom{N}{m}\; \pxxx(\yA),\qquad
  % \E\condt[\big]{\tbinom{\ya}{m}\;\tbinom{n}{m}^{-1}}{\yH} =
  % \E\condt[\big]{\tbinom{\yA}{m}\;\tbinom{N}{m}^{-1}}{\yH},
  m \in \set{1,\dotsc,n}.
%\end{split}
\end{equation}
% where $\pxxx(\yA)$ is our distribution of belief about the larger-population
% activity, and $\pxx$


\medskip


In step~\ref{item:maxent_step} we actually use the
\emph{minimum-relative-entropy} method
\citep{hobsonetal1973,csiszar1985}[\sect~5.2.2]{sivia1996_r2006} with
respect to a uniform reference distribution. We'll still call it
\enquote{maximum-entropy} for brevity. It amounts to two prescriptions:
first, take the distributions satisfying specific convex constraints --
such as fixed expectations -- and among them select the one having minimum
relative entropy with respect to a reference distribution; second, judge
those expectations to be equal to some measured averages, typically time
averages.

In our case we don't know the time averages of the quantity
$\binom{\yA}{m}$, so we cannot directly equate them to the factorial moment
$\E\bigl[\binom{\yA}{m}\bigr]$ of the larger-population distribution
$\pxxx(\yA)$. But \eqn~\eqref{eq:equal_m_factorialmoments} of
step~\ref{item:sample_step} comes to our rescue, because it says that the
expectation for the larger population are determined by that for the sample
$\E\bigl[\binom{\ya}{m}\bigr]$, and we do have the time average of its
corresponding sample quantity $\binom{\ya}{m}$. So we can combine the two
steps:
\begin{equation*}
  \label{eq:chain_prescription_moments}
  \mathrlap{\overbracket{\phantom{\text{measured averages} \rightarrow
        \text{sample moments}}}^{\textit{maximum-entropy prescription}}}
  \text{measured averages} \rightarrow
        \underbracket{\text{sample moments}
  \rightarrow
    \text{larger-population moments}}_{\textit{sampling theory}}
\end{equation*}
We obtain a distribution $\px(\yA \| M, N)$ for the larger population of
$N$ neurons by constraining some of its factorial moments, for example
$M=\set{1,\dotsc,m'}$ with $m'\le n$, to be equal to the sample's recorded
averages. In formulae, the constraints on $\pxxx(\yA)$ are
\begin{equation}
  \label{eq:final_constraint}
  \underbracket{\frac{1}{T}\sum_{t}\binom{\ya_{t}}{m}\bigg/\binom{n}{m}
  \equiv
   \sum_{\ya}
  \binom{\ya}{m}\bigg/  \binom{n}{m} \; \yff_{\ya}}_{\text{measured averages}}
  \mathrel{\;\;=\;\;}
   \underbracket{\sum_{\yA}
     \binom{\yA}{m}\bigg/  \binom{N}{m} \; \pxxx(\yA)}_{\text{distribution moments}}
   \qquad m\in M.
\end{equation}
\smallskip

The result is the distribution of the form
\begin{equation}
  \label{eq:app_maxent_pop_first}
  \begin{gathered}
  \px(\yA \| M, N)  = \frac{1}{Z(\yll)}\;
%  g(\yA)\;
  \exp\Biggl[\sum_{m}\yl_{m}\;
  \binom{\yA}{m}\bigg/\binom{N}{m}
  \Biggr]
  \\
\text{with}\qquad  Z(\yll)\defd \sum_{\yA}
  % g(\yA)\;
  \exp\Biggl[\sum_{m}\yl_{m}\;
  \binom{\yA}{m}\bigg/\binom{N}{m}
  \Biggr],
\end{gathered}
\end{equation}
where the $m'$ parameters $\yll \defd (\yl_{m})$ are determined by the
constraints~\eqref{eq:final_constraint}. This formula is further discussed
in appendix~\ref{sec:derivation_maxent_fullnet}.

The amount and degrees of the constraining moments depend on the questions
and hypotheses that a researcher is exploring. We give some examples in the
next two sections. Note that the constraint
equation~\eqref{eq:final_constraint} for the
distribution~\eqref{eq:app_maxent_pop_first} may not have exact solutions
if $N$ is strictly larger than $n$. The possible discrepancy between its
two sides typically increases with the number of constraints. This
discrepancy comes from an approximation implicit in the maximum-entropy
method combined with one assumption specific to our application: that
measured averages equal expected values (or equivalently, the number of
time bins $T$ is infinite), and that an activity level $\yA$ can equally
likely be generated by any set of $\yA$ neurons in the larger population.
The magnitude of this discrepancy can be a signature of the presence of
neuronal \enquote{assemblies} or \enquote{clusters}
\citep[\chap~12]{gerstneretal2014}{hebb1949_r2002}. We discuss this matter
in \sect~\ref{sec:assumptions}.

\bigskip

One important question remains about the distribution $\px$ of
\eqn~\eqref{eq:app_maxent_pop_first}: \emph{what} is it a distribution of?
-- Of probability? Of relative frequency? The answer is related to an
important fact about the maximum-entropy method, and also to our
alternative point of view of the present model.

% Before applying the formula above to two concrete data sets we want to add
% two remarks about the maximum-entropy method that are seldom made in the
% neuroscientific literature. They are important for the interpretation of
% the results and are further discussed in \sect~\ref{sec:assumptions}.
% First, the maximum-entropy
% method % (based on the \emph{Shannon} relative
% % entropy, as opposed to other entropy functions such as Burg's
% % \citeyear{burg1975})
% rests on some implicit assumptions about the probabilities for the long-run
% frequency distribution of activities, besides the assumptions just
% mentioned at the end of the previous paragraph. So the often-heard
% statement that it gives \enquote{the maximally unbiased (or non-committal)
%   distribution} must be taken with a grain of salt.
% \citep{jaynes1986d_r1996,portamana2009,portamana2017}. Second,
A maximum-entropy distribution like $\px(\yA \| M, N)$ is equivalent to the
zeroth-order approximation (in the sense of Laplace's approximation
\citep[\chap~4]{debruijn1958_r1961}{tierneyetal1986,strawderman2000}) of
four distinct distributions for the larger population, which differ
numerically from one another in higher-order approximations:
\begin{enumerate}[label=(\roman*)]
\item\label{item:prob_freq} the most probable \emph{frequency} distribution
  for the total activity across the \emph{recorded} bins, %$\yFF(\yA)$,
\item\label{item:prob_binin} the \emph{belief} distribution for the value
  of the total activity at any time bin among the \emph{recorded} ones,
  %$\yp(\yA_{t} \| \yH)$ with $1\le t \le T$,
\item\label{item:prob_longrun} the most probable \emph{frequency}
  distribution for the total activity in a very long run of \emph{new} time
  bins, %$\ynuu(\yA)$,
\item\label{item:prob_binout} the \emph{belief} distribution for the value
  of the total activity at a \emph{new} time bin.
  % $\yp(\yA_{t} \| \yH)$ with $t<1$ or $t>T$
\end{enumerate}
Their mathematical relationship is presented more explicitly in
appendix~\ref{sec:inference_prob}. The
distribution~\eqref{eq:app_maxent_pop_first} can therefore be interpreted
in each of the four ways above, in this approximate sense. In the present
case the validity of the approximation decreases as the ratio $nN/T$
increases. The most robust interpretation is~\ref{item:prob_freq}; and this
is how we prefer to interpret the distribution $\px$: as \emph{the most
  probable relative-frequency distribution} of the activity $\yA$. See
\sect~\ref{sec:assumptions} for further discussion about assumptions and
approximations.


\section{Example application: two data sets}
\label{sec:application}

We apply the model just described to two data sets publicly available in
the literature:
\begin{itemize}[wide]
\item The \textbf{first data set}, from \textcite[rat
  14147]{stensolaetal2012}, consists of $n=65$ neurons from rat Medial
  Entorhinal Cortex, recorded for about 20 minutes. Their spikes are binned
  into $T=417\,641$ bins of $3\;\textrm{ms}$ width.
\item The \textbf{second data set}, from \textcite[data courtesy by A.
  Riehle and T. Brochier]{rostamietal2016_r2017}, %\texttt{data\_RS.npy}
  consists of $n=159$ neurons from macaque Motor Cortex, recorded for about
  15 minutes. Their spikes are binned into $T=300\,394$ bins of
  $3\;\textrm{ms}$ width.
\end{itemize}

% In the followingFrom a Bayesian point of view, the maximum-entropy distribution $\px(\yA)$
% can be considered as \emph{the most probable frequency distribution} for
% the larger-population activity during the recording. But it also approximates
% three other distributions, as discussed in the previous section.

\bigskip

The maximum-entropy distribution for the larger population is calculated
using five moments. This number seems to provide almost as much information
as the full frequency distribution of the sample (see next section).
Figure~\ref{fig:plots_5mom} shows the resulting densities
(distribution${}\times N$) for three example values of larger-population
sizes: $N=1\,000$ (\textcolor{mygreen}{green diamonds}), $N=5\,000$
(\textcolor{myred}{red circles}), $N=10\,000$ (\textcolor{myblue}{blue
  curve}). The frequency density of the sample activity is also shown
(\textcolor{black}{black triangles}), and in the plot it would be
indistinguishable from the maximum-entropy density for $N=n$, that is,
applied at the sample level. We discuss the case of unknown $N$ in
\sect~\ref{sec:N}.
\begin{figure}[!p]
  \centering
\subcaption[]{\label{fig:plots_5mom_a}}%  
\includegraphics[width=\linewidth]{_scripts/newpaper_stensola_5mom_distributionsN_priorc.pdf}%
\\[2em]%
\subcaption[]{\label{fig:plots_5mom_b}}%  
\includegraphics[width=\linewidth]{_scripts/newpaper_riehle_5mom_distributionsN_priorc.pdf}%
\\[2em]%
\caption{Maximum-entropy distributions for the normalized total activity of
  the larger population, assuming several population sizes $N$. Five
  moments are constrained.}
\label{fig:plots_5mom}
\end{figure}
% maxent_solutions_different_moments_prior2_6moments.nb

The most expensive calculation, for $N=10\,000$, takes less than 15 minutes
on a laptop with two $2\;\textrm{GHz}$ cores. In all cases the moments were
recovered with relative errors smaller than $10^{-12}$.
% 1000:  3*10^-13 \%
% 5000: 7*10^-13 \%, 581 s
% 10000: 6*10^-11 \%, 737 s

The figure shows that the distribution for the larger-population is more
peaked than the measured frequency distribution for the sample; their
difference increases with $N$. Most remarkably, for the first data set,
\fig~\ref{fig:plots_5mom_a}, the distribution for the larger population has
two distinct low-activity modes. For the second data set,
\fig~\ref{fig:plots_5mom_b}, the distribution presents a small shoulder on
the right of its mode.
% , suggestive of two activity regimes. We frame no hypotheses about the
% biological cause of these two modes (they could stem from the presence of
% different kinds of cells or modules)
Such features are clearly not present in the sample frequencies or in the
maximum-entropy distribution at the sample level. The model thus suggests
interesting features of the larger population. \textcolor{white}{If you
  find this you can claim a postcard from us.}

\begin{figure}[!p]
\centering
\subcaption[]{\label{fig:plots_cellsubsets_a}}%  
\includegraphics[width=\linewidth]{_scripts/newpaper3_gridvsnogrid.pdf}%
\\[2em]%
\subcaption[]{\label{fig:plots_cellsubsets_b}}%  
\includegraphics[width=\linewidth]{_scripts/newpaper3_gridconv.pdf}%
\\[2em]%
\caption{\subcaptionref{fig:plots_cellsubsets_a} Maximum-entropy
  distributions for larger subpopulations or grid and non-grid cells.
  \subcaptionref{fig:plots_cellsubsets_b} Convolution of the two
  subpopulation distributions; it would be equal to the total distribution
  if the two distributions were independent.}
\label{fig:plots_cellsubsets}
\end{figure}
% maxent_solutions_different_moments_partial.nb
Here is a toy example of possible uses of this model,
based on the first data set. We could be interested in the hypothesis that
two distinct cell types or assemblies be present in the region where the
recording was made. Finding a larger-population distribution with two
peaks, as in \fig~\ref{fig:plots_5mom_a}, would provide
some evidence for this hypothesis. Let's further imagine that we have
reasons for suspecting that a specific set of the sampled neurons is of the
first type, and the remaining of the second type. In the case of the first
data set, 27 of the 65 sampled neurons were identified as grid cells
belonging to 3--4 functional modules \citep{dunnetal2015}. Could the
two peaks in the distribution of \fig~\ref{fig:plots_5mom} reflect the
activities of grid versus non-grid cells? We apply the model to these two
sets of neurons individually, using $\yNg=4\,150$ for the grid set and
$\yNng=5\,850$ for the non-grid set, to reflect their proportions (27/65
and 38/65) in the recorded sample. The results are shown in 
 \fig~\ref{fig:plots_cellsubsets_a}. The distribution for the larger
population of grid cells (\textcolor{mygreen}{green triangles}) seems to
have one broad peak, close to the first peak of the larger-population
distribution. The distribution for the larger population of non-grid cells
(\textcolor{myred}{red circles}) has two peaks instead, roughly at the same
normalized activities as the peaks of the larger-population distribution
(\textcolor{myblue}{blue curve}) but closer in height. It would thus seem
that the population of grid cells is contributing to the first mode of the
larger population, but it is not its sole contributor. We can also assess
if the distributions for the two sets of neurons are independent. If they
were independent, the larger-population distribution would be given by
their convolution:
\begin{equation}
  \label{eq:convolution}
  P_{\text{full}}(\yA) = \sum_{\yA'} P_{\text{grid}}(\yA')\;
  P_{\text{non-grid}}(\yA-\yA') %, \qquad \yA \in \set{1,\dotsc,10\,000}
\end{equation}
where the index $\yA'$ runs from $\max(0,\yA-\yNng)$ to $\min(\yA,\yNg)$.
But this is not the case: 
figure~\ref{fig:plots_cellsubsets_b} shows that such convolution
(\textcolor{black}{black diamonds}) is quite different from the
larger-population distribution (\textcolor{myblue}{blue curve}): the two
peaks of the former are closer in position and height than those of the
latter. The population distributions of grid and non-grid cells are
therefore \emph{not} independent: knowledge of the activity of either set
gives us some information about the activity of the other.

\medskip

The toy analysis above should not be taken literally, but just as an
illustration of the model's possible applications. The important point is
that this model is computationally very cheap and yet it can provide
useful insights, even if just qualitative ones, and even suggest new
hypotheses.




\section{Quantifying the importance of higher-order correlations:\\
  the limitations of models at the sample level}
\label{sec:marginalization}

As mentioned in the Introduction (see references there), in the
neurosciences the maximum-entropy method has also been used as a way of
quantifying the \enquote{cooperativity} \citep[\eg][]{gersteinetal1985} or
\enquote{interaction}
\citep[\eg][]{martignonetal1995,schneidmanetal2006,shlensetal2006} or
\enquote{synchrony} \citep[\eg][-- we're only citing early papers using
these terms]{bohteetal2000,amarietal2003} of neuronal activity. Most, if
not all, such applications use the method \emph{at the sample level}. In
this section we discuss how our proposed application bears on this kind of
quantification and compare it with the traditional application at the
sample level. But such a comparison involves some methodological
caveats which we wish to discuss first.% For concreteness we use an
% example with specific statistics: first and second moments (equivalent to
% mean and correlations), versus first-to-fourth moments. But our
% discussion holds for any sets of statistics.

\bigskip

\enquote{Cooperativity}, \enquote{interaction}, and similar terms are
vague, so we need to translate them into a more precise notion first. Here
we use the notion of \emph{informational sufficiency}
\citep[\sect~4.5]{bernardoetal1994}[\chap~8 \amp\
\sect~14.2]{jaynes1994_r2003}{cifarellietal1982,kullbacketal1951}[the
notion goes back to][]{fisher1922} because it relates to those terms, is
intuitive, and is connected with maximum-entropy distributions. Its idea is
as follows. Our probabilities about the frequencies of the activities of
the sample, or about the activity of the sample in a new time bin, are in
principle conditional on all experimental data and statistics we have. But
it can be the case that discarding part of the data or statistics -- for
example, the third- and higher-order empirical moments -- leaves our
probabilities almost unchanged. This means that the discarded statistics
are \emph{informationally irrelevant} or almost so. The remaining
statistics -- for example, first and second empirical moments -- are
\emph{informationally sufficient}.\footnote{For more technical results and
  connections with the notion of symmetry see \eg\
  \cite{darmois1935,neyman1935,koopman1936,pitman1936,halmosetal1949,bahadur1954,berk1972,lauritzen1974,lauritzen1982_r1988,lauritzen2007,cifarellietal1980,cifarellietal1981,diaconisetal1981,diaconis1992,furmanczyketal1998,fortinietal2000,nogalesetal2000,kallenberg2005,ayetal2015}.}
% This informational quasi-sufficiency is interesting because
% it may give clues about population organization and dynamics.

There's a tight connection between informational sufficiency and
maximum-entropy distributions
\citep{jaynes1982b}[\sect~4.5.4]{bernardoetal1994}: if a probability
distribution for repetitive phenomena has a sufficient statistics, then by
the Pitman-Koopman theorem \citep{koopman1936,pitman1936,darmois1935}[for
later analyses and the discrete case
see][]{hipp1974,andersen1970,denny1967,denny1972,fraser1963,barankinetal1963,barndorffnielsen1978_r2014}
it is a mixture of exponential distributions of maximum-entropy form.



We can thus quantify the informational relevance of a subset of statistics,
for example first and second moments (means and correlations), with respect
to a larger set, for example the first four moments, by comparing the
probabilities conditional on the subset and on the full set. For
probabilities built with the maximum-entropy method, this means comparing
those constrained on the subset and on the full set. This procedure is
actually equivalent to comparing the probabilities of the two hypotheses
about sufficiency, conditional on the \emph{full} data, assuming their
pre-data probabilities to be equal. We show this equivalence below and
perform the calculations for our first data set.
\medskip

Before applying this notion to neuronal activity, however, we must keep in
mind that \emph{informational sufficiency is not preserved under sampling}.
If a probability distribution has some sufficient statistics, then its
marginals, such as the distribution for a sample, \emph{cannot} have the
same sufficient statistics, and vice versa; except for trivial cases such
as uniform probability distributions. % This generally holds also among
% different marginals: if a statistics is sufficient for some marginal, then
% it will not be sufficient for another marginal.
This impossibility is known in statistical mechanics: if a system is
described by a Gibbs state, its subsystems cannot be perfectly described by
Gibbs states \citep[\eg][and references therein]{maesetal1999}.
Mathematically this impossibility comes from the Pitman-Koopman theorem
mentioned above, and translates into the general impossibility of solving a
system of independent equations with more equations than unknowns
\citep[\sect~3.1]{portamanaetal2015}.

This fact is important for our analysis. If, say, means and pairwise
correlations seem informationally sufficient for a particular sample from a
brain area, then they may well not be sufficient for the larger population
of neurons constituting that area, and vice versa. So if we are interested
in the \enquote{cooperativity} or \enquote{interaction} of a brain area, it
is unreliable to use a maximum-entropy distribution constructed only for a
sample
thereof. % \citep[contrast this with some literature, \eg][]{martignonetal1995,bohteetal2000,schneidmanetal2006,shlensetal2006,barreiroetal2010,ganmoretal2011,granotatedgietal2013}
The model presented here avoids this problem because the maximum-entropy
method is applied to obtain the distribution of the larger population, not
of the sample alone.

\bigskip

Let us illustrate the remarks above with our first data set.

We measure the difference $\ydi(M'',M')$ in informational sufficiency
between a set of moments, say $M''\defd\set{1,\dotsc,m''}$, and another, say
$M'\defd \set{1,\dotsc,m'}$, as follows:
\begin{enumerate}[label=(\roman*)]
\item from each maximum-entropy distributions $\px(\yA \| M,N)$ for the larger
  population, built from each set of constraints $M=M',M''$, calculate the
  marginal distribution for the sample:
\begin{equation}
  \label{eq:sample_marginal}
  \pxx(\ya \| M, N) = \sum_{\yA}\yG_{\ya \yA}\px(\yA \| M,N),
  \qquad M=M',M'';
\end{equation}
\item calculate the relative entropies of the measured frequency
  distribution $\yf$ with respect to each sample marginal, and multiply
  them by the number of time bins $T$:
  \begin{equation}
    \label{eq:relentropy_T}
    T\,\sh[\yf; \pxx(\ya \| M,N)] \defd
    T  \sum_{\ya} \yff_{\ya} \log\frac{\yff_{\ya}}{\pxx(\ya \| M,N)},
    \qquad M=M',M'';    
  \end{equation}
\item take the difference: 
\begin{equation}
  \label{eq:measure_suffic}
  \begin{split}
  \ydi_{N}(M'',M') &\defd
  T \,\sh[\yf; \pxx(\ya \| M',N)]
  -
  T \,\sh[\yf; \pxx(\ya \| M'',N)]
  \\ &\equiv
T  \sum_{\ya} \yff_{\ya} \log\frac{
  \sum_{\yA}\yG_{\ya\yA}\px(\yA \| M'',N)}{
  \sum_{\yA}\yG_{\ya\yA}\px(\yA \| M',N)}.
\end{split}
\end{equation}
\end{enumerate}
The measure $\ydi_{N}(M'',M')$ so defined is positive if $M''$ is \enquote{more
  informationally sufficient} than $M'$, and negative otherwise.

Why is this a natural measure? Because $\ydi_{N}(M'',M')$ is equal to the
log-ratio of the probabilities of the data $\yf$ conditional on the
hypotheses $M''$ and $M'$:
\begin{equation}
  \label{eq:bayes_factor}
  \ydi_{N}(M'',M') =
  \log\bigl[\pf(\yf \| M'',N)/
  \pf(\yf \| M',N)\bigr].
\end{equation}
This is called their \emph{relative weight of evidence}, the logarithm of
their \emph{relative Bayes factor}
\citep[\chap~6]{good1950}{good1975,good1981,good1985,good1983}[\sect~1.4]{osteyeeetal1974}{mackay1992,kassetal1995}[see
also][p.~421]{jeffreys1936}[\chaps~V, VI, A]{jeffreys1939_r1983}. We prove
this equality in appendix~\ref{sec:measure_suff}. The exponential of
$\ydi_{N}(M'',M')$ tells us how much more probable the data $\yf$ are
conditional on $M''$, than conditional on $M'$. We can also combine this
measure with pre-data probabilities for the two hypotheses to obtain the
ratio of their probabilities conditional on the data
\citep[\cf][]{bretthorst2013}.

% NUMERICAL RESULTS
%
% relentropies of sample2mom, sample4mom, sample5mom, marg2mom, marg4mom, marg5mom
% -322.72,-42.8005,-39.4917,-85.7626,-4.23725,-4.20059 nats =
% -1401.56,-185.88,-171.51,-372.462,-18.4021,-18.2429 dB
%
% difference in relentropies between 4 mom and 2 mom:
% sample level: 279.92 nats = 1215.68 dB
% marg level: 81.525 nats =  354.06 dB
% difference in relentropies between 5 mom and 4 mom:
% sample level: 3.30882 nats = 14.37 dB
% marg level: 0.0366623 nats = 0.159223 dB
% correspond to prob ratios of
% sample: 3.69524\times 10^{121}, 27.3529
% marg: 2.5469\times 10^{35}, 1.03734
% 
% comparison N vs n:
% 2 mom: 236.958 nats = 1029.09 dB, 8.11758\times 10^{102}
% 4 mom: 38.5632 nats = 167.478 dB, 5.59496*10^16
% 5 mom: 35.2911 nats = 153.267 dB, 2.12185*10^15
%
% comparison_ME_sample_full_levels.nb
%

\bigskip

In our case consider for example three sets of constraints $M_{2}$,
$M_{4}$, $M_{5}$, consisting of the first two, four, five moments. For a
larger population of size $N=10\,000$, we obtain the following differences:
\begin{equation}
  \label{eq:diff_suff_N}
  \begin{split}
  \ydi_{N}(M_{4},M_{2}) &= 81\;\nat = 35\;\hart,
\\    \ydi_{N}(M_{5},M_{4}) &= 0.037\;\nat = 0.016\;\hart,
  \end{split}
\end{equation}
where the Hartley ($\hart$) denotes base-10 logarithms
\citep[\sect~C.4]{iso2009}[it was called \enquote{ban} and used by Turing
and Good in their code-breaking work at Bletchley
Park:][]{good1985,good1950,good1969}[\sect~4.2]{jaynes1994_r2003}. In
words, the measured frequencies of the sample activity are 35 orders of
magnitude more probable assuming sufficiency of the first four moments than
assuming sufficiency of the first two only. But they are about as probable
($10^{0.016} = 1.04$) assuming sufficiency of the first five moments as of
the first four moments only.

\begin{figure}[!p]
\centering
\subcaption[]{\label{fig:plots_2_4mom_a}}%  
\includegraphics[width=\linewidth]{_scripts/newpaper2_stensola_twomom_distributions10000_priorc.pdf}%
\\[2em]%
\subcaption[]{\label{fig:plots_2_4mom_b}}%  
\includegraphics[width=\linewidth]{_scripts/newpaper2_riehle_twomom_distributions10000_prior2.pdf}%
\\[2em]%
\caption{Maximum-entropy distributions for the normalized total activity of
  the larger population, assuming two different sets of constraining moments
  and a larger population size $N=10\,000$.}
\label{fig:plots_2_4mom}
\end{figure}
% maxent_solutions_different_moments_prior2_6moments.nb
These informational differences shows clearly in the shapes of the
distribution themselves, plotted in 
\fig~\ref{fig:plots_2_4mom_a}. The two-moment distribution
(\textcolor{myred}{dashed red}) is unimodal, the four-moment distribution
(\textcolor{myblue}{solid blue}) is bimodal. The five-moment distribution
would not be distinguishable from the four-moment one.  Figure~\ref{fig:plots_2_4mom_b} shows the corresponding distributions for the second data set;
also in this case they are visually very distinct.

\medskip

Compare the results above with those obtained by applying the model at the
sample level, that is, with $N=n$:
\begin{equation}
  \label{eq:diff_suff_n}
  \begin{split}
  \ydi_{n}(M_{4},M_{2}) &= 280\;\nat = 1\,220\;\hart,
\\    \ydi_{n}(M_{5},M_{4}) &= 3.3\;\nat = 1.4\;\hart.
\end{split}
\end{equation}
The data are 1\,220 orders of magnitude more probable conditional on four
moments than conditional on two, and $10^{1.4}=25$ times more probable
conditional on five moments than on four moments.

The conclusions about \enquote{cooperativity} or \enquote{interaction} that
we reach by applying the maximum-entropy method to the larger population,
as proposed here, are therefore different from those applying it at the
sample level -- and also visually clearer. The plot in 
\fig~\ref{fig:plots_2_4mom_a}, showing the two clearly different
distributions constrained by two and four moments, should be compared with
the plot for the distributions obtained at the sample level under the same
constraints, shown in \fig~\ref{fig:comparisons_marginals} as
\textcolor{myred}{red circles} and \textcolor{myblue}{blue squares}. The
measured frequencies (black triangles) are also shown. It is necessary to
use a logarithmic scale to see the differences, which appear mainly in the
tail.
\begin{figure}[!p]
\centering
% \includegraphics[width=\linewidth]{_scripts/comparison_full.pdf}%
% \\[3em]%
\includegraphics[width=0.95\linewidth]{_scripts/trunct_comparison_marginals.pdf}%
\caption{Marginals for the normalized total activity of the sample,
  obtained from the maximum-entropy distributions with $N=10\,000$ from
  \fig~\ref{fig:plots_2_4mom_a}, and from maximum-entropy distributions
  calculated at the sample level, that is with $N=n=65$, with two and four
  moments in either case.}
\label{fig:comparisons_marginals}
\end{figure}
% comparison_ME_sample_full_levels.nb

\medskip

% 4 mom: 38.5632 nats = 167.478 dB, 5.59496*10^16
% 5 mom: 35.2911 nats = 153.267 dB, 2.12185*10^15

It is also possible to use the measure~\eqref{eq:measure_suffic} and its
probabilistic meaning~\eqref{eq:bayes_factor} to compare the probability of
the data $\yf$ conditional on the hypothesis $(M_{4},N)$ of four-moment
sufficiency at the larger-population level, $N=10\,000$, versus the
hypothesis $(M_{4}, n)$ of four-moment sufficiency at the sample level,
$N=n=65$. We obtain
\begin{equation}
  \label{eq:diff_suff_n}
  \ydi[(M_{4}, N), (M_{4}, n)] = 39\;\nat = 17\;\hart,
\end{equation}
that is, the data are 17 orders of magnitude more probable under the first
hypothesis than under the second. This result is also shown in
\fig~\ref{fig:comparisons_marginals}, where we see that the four-moment
distribution constructed at the larger-population level
(\textcolor{myblue}{blue filled squares}) fits the measured frequency
distribution (black triangles) more closely than the corresponding
sample-level distribution (\textcolor{myblue}{blue empty squares}).




% But does the application at the larger-population level lead to appreciably
% different results from that at the sample level? after all we're interested
% in an approximate informational sufficiency, not in an analytically exact
% one. A correct answer can only be given case by case.
% Figure~\ref{fig:comparisons_marginals} gives a graphical answer to this
% question in the case of our first data set. The upper plot shows the
% maximum-entropy distributions for a larger population of $10\,000$ neurons,
% constructed from two moments (\textcolor{mygreen}{\textbf{--~--~--}}) and
% from four moments (\textcolor{mypurpleblue}{\textbf{------}}). The lower
% plot shows the maximum-entropy distributions for the sample, from two
% moments (\textcolor{mygreen}{$\bm{\blacktriangledown}$}) and from four moments
% (\textcolor{mypurpleblue}{$\bm{\square}$}), and the distributions obtained
% by marginalizing the larger-population distribution to the sample size, from two
% moments (\textcolor{mygreen}{$\bm{\blacktriangledown}$}) and from four
% moments (\textcolor{mypurpleblue}{$\bm{\blacksquare}$}). The measured
% frequency distribution (\textcolor{myred}{\LARGE$\bm{\bullet}$}) is also
% shown. We note the following:
% \begin{itemize}
% \item The application to the larger-population (upper plot) leads to two
%   completely different distributions (even different number of modes);
%   clearly the two sets of statistics are not even approximately equivalent
%   for inferential purposes.
% \item The application at the sample level leads to deceivingly similar
%   distributions (\textcolor{mygreen}{$\bm{\blacktriangledown}$} and
%   \textcolor{mypurpleblue}{$\bm{\square}$}), which can only be distinguished on a
%   logarithmic scale. This could lead to the erroneous conclusion that the
%   two statistics are approximately equivalent.
% \item The difference between marginals from the larger-population distribution
%   and the distributions obtained at the sample level (filled vs empty
%   markers) is larger than the difference between different statistics
%   (triangles vs squares).
% \item The marginals of the application to the larger population are closer to
%   the measured frequencies than the distributions obtained at the sample
%   level (although this closeness is not a valid criterion for their
%   goodness).
% \end{itemize}
% Therefore, the maximum-entropy application at the sample level and at the
% larger-population level lead to different results; the latter is not only more
% meaningful but also superior because it shows clearly the different
% informational sufficiency of the two sets of statistics.


\section{The size $N$ of the larger population}
\label{sec:N}

% results for post-data probabilities of N given data and 5 moments:
% N=65,1000,2000,5000,10000
% 1.40169\times 10^{-16}, 0.0439602, 0.139657, 0.251064, 0.297726, 0.267594
%
% using 1/N pre-data prob:
% 1.04099\times 10^{-14}, 0.212211, 0.337085, 0.242394, 0.143722, 0.0645882
% 
% maxent_solutions_different_moments_prior2_6moments.nb

The calculations and conclusions presented in the preceding sections depend
on the size $N$ of the larger population. The larger population can't be
the whole brain, of course. How large should or can $N$ be in our formulae?

The crucial point is our belief distribution for the activity of the sample
\emph{conditional} on the activity of the larger population. It is given by
the hypergeometric distribution~\eqref{eq:conditional_hypergeometric},
reprinted here:
\begin{equation}
  \labelbis{eq:conditional_hypergeometric}
  \yG_{\ya\yA} \defd \pf(\ya \|\yA, n,N)=
    \binom{\yA}{\ya}\binom{N-\yA}{n-\ya}\binom{N}{n}^{-1}
\equiv \binom{n}{\ya}\binom{N-n}{\yA-\ya}\binom{N}{\yA}^{-1}.
\end{equation}
This distribution leads to the equality of factorial
moments~\eqref{eq:equal_m_factorialmoments}, on which our model rests. This
conditional probability, characteristic of \enquote{drawing without
  replacement}
\citep[\chap~3]{jaynes1994_r2003}[\sect~4.8.3]{ross1976_r2010}[\sect~II.6]{feller1950_r1968},
ensues when our degree of belief that the $i$th unit (ball or neuron) will
be drawn has the same value for all $i \in \set{1,\dotsc,N}$. Thus,
consider the pool of all neurons which we believe -- with equal degree for
each neuron -- could have been recorded. $N$ is the size of that pool. $N$
therefore depends on factors such as: the shape, dimensions, and technical
specifications of the recording probe; the inaccuracy in the insertion of
the probe, leading for example to slightly different insertion points or
angles; the density of neurons around the probe. But it also depends on the
homogeneity of the brain region where the recording was made: if we believe
that it doesn't matter whether the probe had been inserted in some other
point of the same brain region, then
formula~\eqref{eq:conditional_hypergeometric} is appropriate. $N$ can
therefore only be assessed case by case.

Whether such equal beliefs are justified, or for which sampling procedures
they can be justified, is the fundamental, deep question of sampling
theory, which we cannot discuss here \citep[see \eg\ the discussions,
reviews, and references
in][]{ericson1969,smith1976,kruskaletal1979c,kruskaletal1980}. It is of
course possible to probabilistically assess, case by case, whether this
equality assumption is appropriate. But we must also be aware that such
assessment in turn rests on analogous assumptions at a higher level, and so
on. The probability calculus, just like the logical calculus, cannot yield
conclusions without premisses \citep[p.~182]{johnson1924}.

Luckily the plots of \fig~\ref{fig:plots_5mom} suggest that the order of
magnitude of $N$ ought to be enough for qualitative inferences. As
discussed in the next section, even if the exact number $N$ were known, the
maximum-entropy distribution ought to be interpreted qualitatively or
semi-quantitatively.

If our uncertainty about $N$ spans several orders of magnitude we can
assign a probability to the possible values of $N$ based on our background
information and the frequency data $\yf$, similarly to the procedure in
\sect~\ref{sec:marginalization} and appendix~\ref{sec:measure_suff}. The
probability for the data $\yf$ conditional on a set of constraints $M$ and
size $N$ is the exponential of the negative relative
entropy~\eqref{eq:relentropy_T}:
\begin{equation}
  \label{eq:prob_data_from_N}
  \pf(\yf \| M, N) = \binom{T}{T\yf}\;\prod_{a}\pxx(\ya \| M, N)^{T\,\yff_{a}}
  \approx \exp\bigl\{  -T\,\sh[\yf; \pxx(\ya \| M, N)]\bigr\}.
\end{equation}
If our pre-data probability for $N$ is $p(N)$, then by Bayes's theorem
\begin{equation}
  \label{eq:prob_N_from_data}
  \pf(N \| M, \yf) \propto
  p(N)\;\exp\bigl\{-T\,\sh[\yf; \pxx(\ya \| M, N)]\bigr\}.
\end{equation}

Here is an illustrative example with our first data set. Suppose our
uncertainty spans slightly more than an order of magnitude, from $N=1\,000$
to $N=20\,000$. Divide this range roughly into thirds of order of
magnitude, considering the values
$N\in \set{1\,000, 2\,000, 5\,000, 10\,000, 20\,000}$. Assuming the set $M$
of first five moments to be sufficient, formula~\eqref{eq:prob_data_from_N}
gives
\begin{equation}
  \label{eq:data1_f_from_N}
  \begin{gathered}
  \pf(\yf \| N=1\,000, M) =  0.00222,\qquad
  \pf(\yf \| N=2\,000, M) =  0.00704,\\
  \pf(\yf \| N=5\,000, M) =  0.0127,\qquad
  \pf(\yf \| N=10\,000, M) =  0.0150,\\
  \pf(\yf \| N=20\,000, M) =  0.0135.
\end{gathered}
\end{equation}
Since our uncertainty regards a scale factor, it can be represented by
equal pre-data probabilities of $1/5$ about these partial orders of
magnitude. Thus from~\eqref{eq:data1_f_from_N} we find
\begin{equation}
  \label{eq:data1_N_from_f}
  \begin{gathered}
  \pf(N=1\,000 \| \yf, M) =  0.044,\qquad
  \pf(N=2\,000 \| \yf, M) =  0.140,\\
  \pf(N=5\,000 \| \yf, M) =  0.251,\qquad
  \pf(N=10\,000 \| \yf, M) =  0.298,\\
  \pf(N=20\,000 \| \yf, M) =  0.267,
\end{gathered}
\end{equation}
which gives a slightly higher probability to $N=10\,000$.

\bigskip

In this way we can make inferences -- for example about the sufficiency of
a set of moments, or about the marginal sample distribution -- that take
into account our uncertainty about $N$. We must make sure to avoid
circularities, though: for example, we can't assume a set of moments $M$ to
be sufficient and assess our uncertainty about $N$ conditional on it, and
then use this uncertainty to assess the sufficiency of $M$. But we can
approximately assess the sufficiency of a subset of moments $M' \subset M$
taking into account the uncertainty of $N$ conditional on $M$.

A rigorous assessment would involve a more expensive, full Bayesian
calculation; but such calculation would make the maximum-entropy
method superfluous. We discuss this final point in the next section.


\section{Assumptions and approximations that define the model}
\label{sec:assumptions}

The possible uses of the model here presented depend on the assumptions and
approximations which define it: three main assumptions and two main
approximation all in all. These become clear within a full-fledged
probabilistic approach. We summarize them here and give a more mathematical
sketch in appendix~\ref{sec:inference_prob}.

The first and most important assumption, typical of this kind of
maximum-entropy application in neuroscience, can be expressed in two
equivalent ways:
\begin{enumerate*}[(\roman*)]
  % \item the order of the time bins has no relevance;
\item the frequencies of the activity during a recording are
  informationally sufficient for inferences about unrecorded times; \item
  our degree of belief about the sequence of activities is invariant under
  permutations of the time bins.
\end{enumerate*}
Equating time averages and expectations, \eqn~\eqref{eq:final_constraint},
would not make sense without this assumption. Its mathematical consequence
is to relate the probability for the frequency distribution $\yF$ during
the recording to that of the \emph{long-run} frequency distribution for the
activities of the larger population. Note that the maximum-entropy method
in statistical physics is meant to be used not with this assumption, but
with averages across \emph{repetitions} of the same experiment, because its
constraints represent macroscopically \emph{reproducible} quantities and
kinematics, even in non-equilibrium
\citep{jaynes1957,guntonetal1983,tikochinskyetal1984,grandy1988,berryetal1988,deroecketal2005_r2006,touchette2009}.

The second assumption, typical of maximum-entropy in general, concerns the
form of the probability distribution for the long-run frequencies ensuing
from the first assumption. It is taken to be either an \enquote{entropic
  prior} \citep{skillingetal1984,rodriguez1991,neumann2007}, which takes
into account the multiplicity of long-run frequencies, or a prior for a
model with sufficient statistics, which expresses the sufficiency of the
moments under study \citep{portamana2017}. This assumption is crucial for
deriving the maximum-entropy based on with the Shannon entropy. A different
density leads to alternative maximum-entropy methods, for example based on
the Burg entropy \citep{burg1975} if a Dirichlet density is used
\citep{jaynes1986d_r1996}. The entropic prior depends on a reference
distribution and on a coefficient $L$ which determines the sharpness of the
prior's peak. The computations of the preceding sections were insensitive
to the reference distribution in their significant digits. We used three
biologically reasonable alternatives: a flat distribution, one linearly
decreasing with $\yA$, and a normal one with a broad peak at low
activities.

The third assumption is specific to our application: all subsets of $\yA$
neurons in the larger population are equally likely to give rise to total
activity $\yA$, at every time bin. A mathematically equivalent assumption
is that $n$ neurons are sampled anew at each time bin. This assumption
allows us to factorize the joint probability of the two sequences of
activities and apply the sampling
relation~\eqref{eq:conditional_hypergeometric} of
appendix~\ref{sec:derivation_maxent_fullnet} at each bin.

Finally, the maximum-entropy method is typically equivalent to
approximating the number $T$ of time bins, the prior coefficient $L$, and
the ratio $T/L$ to infinity. These approximations lead to the
identification of the four distributions listed at the end of
\sect~\ref{sec:model}: the recorded frequency distribution becomes equal to
the long-run one, and also to the probability distribution for the activity
at recorded and new time bins. In our application it also leads to the
equality of the hypergeometric
distribution~\eqref{eq:conditional_hypergeometric} and the conditional
frequencies of the sample activity $\ya$ given $\yA$. The goodness of this
approximation decreases as the ratio between the number of possible joint
states and the number of bins -- roughly $nN/T$ -- increases, and also as
observed quantities approach their mathematical bounds, such as some
frequencies $\yff_{\ya}=0$. Without these approximations it is still
possible to interpret the maximum-entropy
distribution~\eqref{eq:app_maxent_pop_first} as the mode of the probability
distribution for the recorded frequencies $\yF$, that is, the most probable
recorded frequency.


\medskip

The first and third assumptions above are the least realistic, but they
make sense as reference hypotheses for judging the informational relevance
of correlations of some order, as in \sect~\ref{sec:marginalization}, to be
compared with hypotheses about more realistic time dependences and
clustering. %They may turn out to approximate well more realistic assumptions.

The $T= \infty$ approximation can break our model for specific
distributions of sample frequencies, if too many moments are used. For
example take $n=2$ with frequency distribution $\yf=(0.5,0,0.5)$ for the
activities $\ya=0,1,2$; that is, the sample is silent half of the time and
completely active half of the time. The first two normalized factorial
moments have both value $0.5$, and together they uniquely determine $\yf$.
If we now take $N=3$, it can be proved that
\eqn~\eqref{eq:app_maxent_pop_first}, constrained on those two moments, has
no solution. The reason is clear from a sampling perspective: if there is
zero probability of sampling one active neuron, then the larger population
cannot have any active neurons, and there must be zero probability of
sampling two active ones. But this reasoning holds only for each single
time bin, not for the averages over time bins. The $T=\infty$ approximation
identifies the two instead. Outside this approximation there are no
contradictions because the conditional frequencies and conditional
probabilities are kept distinct.

The two infinity approximations are at the limit of their validity in our
examples. For this reason we interpret the distributions as \emph{most
  probable frequency distributions}, as mentioned at the end of
\sect~\ref{sec:model}. The space of possible frequency distributions for
the larger population has $N$ dimensions, however, and probability
distributions in high dimensions have counter-intuitive properties. For
example, the mode or mean can have \emph{atypical} features when compared
with the majority of other probable points of the probability space. The
question then is \emph{which features of the maximum-entropy frequency
  distribution are typical of the majority of plausible frequency
  distributions?} Preliminary studies using a full probability calculation
(to be published separately) show that several semi-quantitative features
of the maximum-entropy distribution are indeed typical. For the first data
set, for example, most frequency distributions have two high-frequency
regimes of activity, as in \fig~\ref{fig:plots_5mom_a}, with roughly the
same heights and almost the same locations, within a few percentages of
normalized total activity. Each of high-frequency regimes, though, may
consist of two or three very close modes rather than one. The same
preliminary studies also indicate that these typical features remain with
prior choices in the second assumption above, for example using a Dirichlet
prior instead of an entropic one.

This means that the maximum-entropy approximation presented here can be
favourably used to get an idea of what a fully probabilistic calculation
could yield. -- With a far lower computational cost: for example, for
$N=1\,000$ the maximum-entropy distribution can be calculated in ten
minutes on a laptop, whereas the full probability calculation takes several
days on a high-performance computing cluster.



% \medskip

% The second point is that our degrees of belief about the frequency
% distribution for the larger population depend not only on the measured data in
% the sample, but also on our pre-data beliefs $\yI$ about the distribution.
% Which assumptions lead to the maximum-entropy result? This distribution
% appears when our initial belief about the possible frequency distributions
% $\yF$ is quantified by an entropic prior
% \citep{neumann2007,rodriguez1991,skilling1998,catichaetal2004,portamana2017}:
% \begin{equation}
%   \label{eq:entropic_prior}
%   \pf(\yF \| \yI) \propto  \exp[-\yL\;\re(\yF;\yR)]
%   \approx \binom{L}{L\yFF_{0}, \dotsc, L\yFF_{N}}\;
%   \prod_{\yA}{\yRR_{\yA}}^{L\yFF_{\yA}}
% \end{equation}
% where $\re(\yF;\yR)\defd \sum_{\yA}\yFF_{\yA}\log(\yFF_{\yA}/\yRR_{\yA})$ is
% the relative entropy or discrimination information
% \citep{kullback1987,jaynes1963,hobson1969,hobsonetal1973}, $\yR$ is the
% reference distribution, and $\yL$ a positive parameter. The approximate
% equality (obtained through Stirling's approximation), where the large
% parentheses denote a multinomial coefficient, shows that this prior belief
% is proportional to the number of ways in which the distribution $\yF$ can
% be realized in $L$ time bins. The parameter $L$ roughly quantifies how many
% time bins our data set must have to affect our initial belief. The
% maximum-entropy approximation is valid when $L$ is large, but small
% compared to the sharpness of the constraints on $\yF$; in our case this
% means $\yL \approx 10$, give or take an order of magnitude.



\section{Summary and discussion}
\label{sec:summary_discussion}

The reason why we record neurons from a brain region is to have an idea of
the activity of the other neurons in that region, similarly to survey
sampling. If we thought that \emph{precisely} the recorded ones were
exceptions or completely unrelated to the others surrounding the recording
probe, our recording would serve little purpose. Probabilistic models that
allow us to make direct inferences about the larger population are
therefore useful. But they become complex and sensitive if we ask for very
detailed inferences.

We have presented a model that allows us to make direct inferences about a
larger population from very small samples. Its inferences regard only the
frequencies of the total activity of the larger population, but such
limited inferential scope makes the model computationally very cheap.

Yet, and most important, this model leads to inferences that are not at all
trivial and not easily discernible by looking at the sample. We showed this
by applying the model to two real data sets.

By \enquote{inference} we mean the quantification of a degree of belief
about possible observations, based on specific assumptions; not the
response of an oracle. The model here presented gives us a \emph{forecast}
based on specific assumptions discussed in \sect~\ref{sec:assumptions}, and
therefore has two main uses, just like any other probabilistic model. If
our assumptions are only hypotheses under testing, then the subsequent
confirmation or confutation of the forecast will increase or decrease our
confidence in those hypotheses. If we strongly trust our assumptions, on
the other hand, then we rely on the forecast and even use it in place of
actual observations, especially if the latter are difficult to make. (In
fact, we typically shift from the first to the second use.) In the
preceding sections we have given examples of additional hypotheses that
could be explored with the present model. The assumptions of the model
itself (\sect~\ref{sec:assumptions}) can either be under test or relied
upon, depending on the case.

Some approximations implicit in the model make its numerical results
semi-quantitative with respect to an exact analysis. The results should not
be off by more than a few percentages for the normalized total activity and
for the frequency density. This imprecision, however, is compensated by the
extreme computational cheapness of the model, far less costly than an exact
analysis. The model is therefore very convenient at least for essaying
hypotheses. We hope to present an exact quantitative comparison with the
full probabilistic approach in a future publication.



% \mynote{cite  \cite[p.~440]{maxwell1873c}}

% We have presented a procedure to construct the most plausible frequency
% distribution of population-averaged activities of a population of neurons,
% given the recording about a small sample thereof. This procedure combines
% the maximum-entropy method and basic identities from sampling theory. From
% the application to two real data sets we saw that the frequency
% distributions obtained with our procedure can have features very different
% from the one measured in the sample, such as multiple modes. This procedure
% can also be used with moment constraints of different order -- means,
% population-averaged pairwise correlations, or higher-order correlations --
% thus giving an approximate assessment of the informational sufficiency of
% specific subsets of moments. In fact, we saw that the application of
% maximum-entropy only at the sample level leads to misleading results about
% this kind of sufficiency questions.




%%\setlength{\intextsep}{0.5ex}% with wrapfigure
%\begin{figure}[p!]%{r}{0.4\linewidth} % with wrapfigure
%  \centering\includegraphics[trim={12ex 0 18ex 0},clip,width=\linewidth]{maxent_saddle.png}\\
%\caption{caption}\label{fig:comparison_a5}
%\end{figure}% exp_family_maxent.nb


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Acknowledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{acknowledgements}
  This work is financially supported by the Kavli Foundation and the Centre
  of Excellence scheme of the  Research Council of Norway (Yasser Roudi group).\\
  PGLPM thanks the staff of the NTNU library for their always prompt
  support; Mari, Miri, \amp\ Emma for continuous encouragement and
  affection; Buster Keaton and Saitama for filling life with awe and
  inspiration; the developers and maintainers of \LaTeX, Emacs, AUC\TeX,
  Open Science Framework, Python, Inkscape, Sci-Hub for making a free and
  unfiltered scientific exchange possible.
%\rotatebox{15}{P}\rotatebox{5}{I}\rotatebox{-10}{P}\rotatebox{10}{\reflectbox{P}}\rotatebox{-5}{O}.
%\sourceatright{\autanet}
\end{acknowledgements}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Appendices
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%\clearpage
% %\renewcommand*{\appendixpagename}{Appendix}
% %\renewcommand*{\appendixname}{Appendix}
% %\appendixpage
\appendix

\section{Derivation of the maximum-entropy distribution}
\label{sec:derivation_maxent_fullnet}

Here is a summary derivation of the maximum-entropy distribution for the
larger population constrained by a set of factorial moments.%  For further details
% see Porta Mana \etal\ \citey{portamanaetal2015}.

\bigskip

First of all the sampling relation. We have a population of $N$ units --
neurons -- $\yA$ of which have some specific property -- being active in a
specific time bin -- and we sample in an unknown way $n$ of the $N$ units.
The probability that $\ya$ of the $n$ sampled neurons are active is then
given by the hypergeometric distribution
\begin{equation}
  \label{eq:conditional_hypergeometric}
  \yG_{\ya\yA} \defd \pf(\ya \|\yA, n,N)=
  \binom{\yA}{\ya}\binom{N-\yA}{n-\ya}\binom{N}{n}^{-1}
\equiv  \binom{n}{\ya}\binom{N-n}{\yA-\ya}\binom{N}{\yA}^{-1}
\end{equation}
typical of \enquote{drawing without replacement}
\citep[\chap~3]{jaynes1994_r2003}[\sect~4.8.3]{ross1976_r2010}[\sect~II.6]{feller1950_r1968}[\sects~2.1,
3.2]{jeffreys1939_r1983}. In the following we leave $n$, $N$ implicit in
the conditional. If we are uncertain about the number $\yA$, with belief
$P(\yA)$, then by the theorem of total probability our belief about $\ya$
is
\begin{equation}
  \label{eq:belief_sample_unknown_A}
  p(\ya)= \sum_{\yA} \yG_{\ya\yA} \;P(\yA).
\end{equation}

From the definition of normalized factorial
moment~\eqref{eq:def_factorial_moment_a}, the expression for the
hypergeometric distribution~\eqref{eq:conditional_hypergeometric}, and the
relation~\eqref{eq:belief_sample_unknown_A} between our beliefs about $\ya$
and $\yA$, and using some combinatorial juggling
\citep[\chaps~I--IV]{whitworth1867_r1965}[\chap~II]{feller1950_r1968}[appendix~A]{portamanaetal2015}{potts1953}, one
can prove the equality~\eqref{eq:equal_m_factorialmoments} between the
factorial moments of sample and larger population.

\bigskip

For the construction of a maximum-entropy distribution from generic
expectation constraints see Jaynes
\citey{jaynes1963}[\chap~11]{jaynes1994_r2003}. More precisely we use the
minimum relative-entropy method
\citep{hobsonetal1973,csiszar1985}[\sect~5.2.2]{sivia1996_r2006} with
respect to a reference distribution. For the solution of the extremization
problem using Lagrangians and Lagrange multipliers see Mead \etal\
\citey{meadetal1984} and the extensive texts by Fang \etal\
\citey{fangetal1997} and Boyd \etal\ \citey{boydetal2004_r2009}. For a
geometric understanding of the extremization and of the relation between
expectations and multipliers see Porta Mana \citey{portamana2017b}.

The result has the standard exponential-family form
\begin{equation}
  \label{eq:app_maxent_pop}
  \begin{gathered}
  \px(\yA)  = \frac{1}{Z(\yll)}\;
  r_{\yA}\;
  \exp\Biggl[\sum_{m}\yl_{m}\;
  \binom{\yA}{m}\binom{N}{m}^{-1}
  \Biggr],
  \\
  Z(\yll)\defd \sum_{\yA} r_{\yA}\;
  \exp\Biggl[\sum_{m}\yl_{m}\;
  \binom{\yA}{m}\binom{N}{m}^{-1}
  \Biggr],
\end{gathered}
\end{equation}
where $(r_{\yA})$ is the reference distribution and $\yll\defd (\yl_{m})$ are
the Lagrange multipliers, satisfying the implicit
constraint equations~\eqref{eq:final_constraint}:
\begin{equation}
  \label{eq:constraint_eq_lambda}
  \sum_{\yA}
  \tbinom{\yA}{m}\tbinom{N}{m}^{-1}\;
  \tfrac{1}{Z(\yll)}\;
  r_{\yA}\;
  \exp\biggl[\sum_{m}\yl_{m}\;
  \tbinom{\yA}{m}\tbinom{N}{m}^{-1}
  \biggr]
=
  \sum_{\ya}
  \tbinom{\ya}{m} \tbinom{n}{m}^{-1} \; \yff_{\ya},\quad
  m\in M.
\end{equation}

The reference distribution $(r_{\yA})$ represents our pre-data beliefs
about the activity levels $\yA$. We know that the majority of neurons in a
brain area are rarely simultaneously active within a window of some
milliseconds, so we could choose a distribution with slightly higher
weights on low values of $\yA$. On the other hand, considering the number
of ways in which $\yA$ out of $N$ neurons can be simultaneously active
would suggest the multiplicity distribution proportional to
$\binom{N}{\yA}$. It turns out that our results of
\sects~\ref{sec:application}--\ref{sec:marginalization} are insensitive to
the choice between these two possible reference distributions, or even a
uniform reference distribution.

For a derivation from a Bayesian perspective see
appendix~\ref{sec:inference_prob}.


\section{Measure of informational sufficiency}
\label{sec:measure_suff}

Let's ask how much more probable is the
sufficiency of one set with respect to the other, conditional on our data
$\yf$:
\begin{equation}
  \label{eq:ratio_prob_suffic}
  \frac{p(M'' \| \yf)}{ p(M' \| \yf)}.
\end{equation}
The probability of observing activity $\ya$ in the sample at any time bin
is the sample marginal of the maximum-entropy distribution for the larger
population, owing to the excheangeability assumption implicit in the
maximum-entropy method:
\begin{equation}
  \label{eq:sample_marginal}
  \pxx(\ya_{t} \| M) = \sum_{\yA}\yG_{\ya_{t}\,\yA}\;\px(\yA \| M).
\end{equation}
The probability of observing one sequence $(\ya_{t})$ with frequencies
$\yf$ is therefore
\begin{equation}
  \label{eq:prob_frequencies}
  \prod_{t=1}^{T} \pxx(\ya_{t} \| M) \equiv
  \prod_{\ya=0}^{n} \pxx(\ya \| M)^{T\,\yff_{\ya}} \equiv
  \prod_{\ya=0}^{n}\biggl[\sum_{\yA}\yG_{\ya\yA}\;\px(\yA \| M)\biggr]^{T\,\yff_{\ya}}.
\end{equation}
The probability of observing the frequencies $\yf$ is obtained multiplying this
by their multiplicity factor, the multinomial coefficient
\begin{equation}
  \label{eq:multiplicity_factor}
  \binom{T}{T\yf} \defd \frac{T!}{\prod_{\ya}(T\,\yff_{\ya})!}
  \approx \prod_{\ya}{\yff_{\ya}}^{-T\,\yff_{\ya}},
\end{equation}
the last expression coming from Stirling's approximation
\citep[Lemma~2.2]{csiszaretal2004b}.
If we assign equal pre-data probabilities to the two hypotheses $M'$ and $M''$,
each probability in the ratio~\eqref{eq:ratio_prob_suffic} then becomes, by
Bayes's theorem,
\begin{multline}
  \label{eq:prob_proportional_relentropy}
  \pf(M \| \yf) \propto
  \pf(\yf \| M) \times\text{\small const} \propto
  \binom{T}{T\yf}\,  \prod_{\ya}\biggl[\sum_{\yA}\yG_{\ya\yA}\px(\yA \| M)\biggr]^{
    T\,\yff_{\ya}}
   \approx{}\\
  \prod_{\ya}{\yff_{\ya}}^{-T\,\yff_{\ya}}
  \times
  \prod_{\ya}\biggl[\sum_{\yA}\yG_{\ya\yA}\px(\yA \| M)\biggr]^{
    T\,\yff_{\ya}}.
\end{multline}
The logarithm of the probability above is easily seen to be the number of
bins $T$ multiplied by relative entropy between the frequency distribution
$\yf$ and the sample marginal of the maximum-entropy distribution.

Thus, the difference~\eqref{eq:measure_suffic} is the logarithm of the
probability ratio~\eqref{eq:ratio_prob_suffic}. The exponential of the
difference~\eqref{eq:measure_suffic} tells us how much more probable the
hypothesis that the set $M''$ is sufficient than the hypothesis that the
set $M'$ is.

\section{Assumptions and approximations: mathematical sketch}
\label{sec:inference_prob}

We give here a sketch of the mathematical formulae behind the assumptions
and approximations discussed in \sect~\ref{sec:assumptions}. We indicate
them all collectively with $\yI$.

The first assumption, of infinite exchangeability
\citep[\chap~4]{bernardoetal1994_r2000}{dawid2013} of the probability
distribution $\pf[(\yA_{1},\dotsc, \yA_{T}) \| \yI]$ for the sequence of
activities, implies the following mixture form by de~Finetti's theorem
\citep{definetti1930,hewittetal1955}:
\begin{equation}
  \label{eq:mixture_from_definetti}
%  \exp[-T\;\sh(\yF;\ynu)] \quad
%  \pxxx[(\yA_{1},\dotsc,\yA_{T})] \propto
  \pf[(\yA_{1},\dotsc, \yA_{T}) \| \yI]
  = \int\!\!\!\di\ynu\;\pf(\ynu \|\yI)\,\prod_{\yA=1}^{N}{\nu_{\yA}}^{T\,\yFF_{\yA}},
\end{equation}
where $\ynu\defd(\nu_{\yA})$ is the long-run frequency distribution of
activities in an imaginary continuation or repetition of the recording, and
$\pf(\ynu \|\yI)\;\di\ynu$ our belief distribution about it.

The second assumption concerns the latter degree distribution. It has one
of these expressions:
\begin{subequations}\label{eq:maxent_prior}
  \begin{equation}
    \label{eq:entropic_prior}
    \pf(\ynu \| \yI) \propto  \exp[-\yL\;\sh(\yF;\yr)]
    \approx \binom{L}{L\nu_{0}, \dotsc, L\nu_{N}}\;
    \prod_{\yA}{r_{\yA}}^{L\nu_{\yA}}  
  \end{equation}
  where $\yr\defd(r_{\yA})$ is a reference distribution and the expression
  in larger parentheses is a multinomial coefficient, or
  \begin{equation}
    \label{eq:suff_prior}
    \pf(\ynu \| \yI) \propto
    \int\!\!\!\di\yla\;\pf(\yla \| \yI)\;
    \prod_{\yA}\delt\Biggl\{\ynu_{\yA}-
    \frac{r_{\yA}}{Z(\yla)}\,\exp\Bigl[
    \tsum_{m}\lambda_{m}\tbinom{\yA}{m}/\tbinom{N}{m}
    \Bigr]\Biggr\},
  \end{equation}
\end{subequations}
where $\yla\defd(\lambda_{m})$ are Lagrange multipliers and $Z(\yla)$ a
normalization factor for the exponential distribution within the delta.
This is a formal expression saying that our belief about $\ynu$ is an
exponential parametric model for which the normalized factorial moments are
informationally sufficient.

The third assumption corresponds to the formula
\begin{multline}
  \label{eq:assume_sampled_anew}
  \pf[(\ya_{1},\dotsc,\ya_{T}) \| (\yA_{1}, \dotsc, \yA_{T}), \yI ]
  ={}\\ \prod_{t=1}^{T} \pf(\ya_{t} \| \yA_{t}, \yI)
  = \prod_{t=1}^{T} \yG_{\ya_{t}\yA_{t}}
  \equiv \prod_{\ya,\yA}{\yG_{\ya_{t}\yA_{t}}}^{T\,J_{\ya\yA}},
\end{multline}
where $J_{\ya\yA}$ is the joint frequency distribution of the activity
pairs $(\ya,\yA)$, so that its marginals are the frequency distributions
$\yf$ and $\yF$. As discussed in \sect~\ref{sec:assumptions} this formula
follows from either of two beliefs: that all subsets of $\yA$ neurons in
the larger population are always equally likely to give rise to total
activity $\yA$, or that a new sampling is done at every time bin.

It can be proved that the formulae above lead to the following belief
distribution for the joint frequency distribution $\yJ \defd (J_{\ya\yA})$
conditional on $\yf$:
\begin{multline}
  \label{eq:prob_joint_frequencies}
  \pf(\yJ \| \yf, \yI) \propto
                         \int\!\!\!\di\ynu\;\pf(\ynu\|\yI)\;
                         \binom{T}{TJ_{0 0},\dotsc,TJ_{n N}}\,
                         \prod_{\ya,\yA}\bigl(\yG_{\ya\yA}\;\nu_{\yA}\bigr)^{T\,J_{\ya\yA}}
  \approx{}  \\
\shoveright{\int\!\!\!\di\ynu\;\pf(\ynu\|\yI)\;
                         \exp\biggl(
-T \tsum_{\ya,\yA}J_{\ya\yA}\ln\frac{J_{\ya\yA}}{\yG_{\ya\yA}\;\nu_{\yA}}
                         \biggr)}
  \\
  \text{with}\quad \tsum_{\yA}J_{\ya\yA}=\yff_{\ya},
\end{multline}
where $\pf(\ynu\|\yI)$ is either of the
distributions~\eqref{eq:maxent_prior}. From this belief distribution we can
find the one for $\yFF_{\yA}\equiv \sum_{\ya}J_{\ya\yA}$ by
marginalization. Within the exponential we can recognize the relative
entropy of $\yJ$ with respect to $(\yG_{\ya\yA}\;\nu_{\yA})$, which is the
reason why these two joint distributions must be equal if $T$ tends to
infinity.

From the formulae above we can also find the four distributions mentioned
at the end of \sect~\ref{sec:model}. The frequency
distribution~\ref{item:prob_freq} is the mode of $\pf(\yF \| \yf,\yI)$. The
belief distribution~\ref{item:prob_binin} is
\begin{equation}
  \label{eq:prob_binin}
  \pf(\yA_{t} \| \yf, \yI) =
  \sum_{\yF} \yFF_{\yA_{t}}\; \pf(\yF \| \yf, \yI),
  \qquad t \in \set{1,\dotsc,T}.
\end{equation}
The frequency distribution~\ref{item:prob_longrun} is the mode of $\pf(\ynu
\| \yf,\yI)$. Finally, the belief distribution~\ref{item:prob_binout} is
\begin{equation}
  \label{eq:prob_binout}
  \pf(\yA_{t} \| \yf, \yI) =
  \int\!\!\!\di\ynu\;\nu_{\yA_{t}}\;\pf(\ynu \| \yf, \yI),
  \qquad t \notin \set{1,\dotsc,T}.
\end{equation}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\defbibnote{prenote}{{\footnotesize (\enquote{de $X$} is listed under D,
    \enquote{van $X$} under V, and so on, regardless of national
    conventions.)\par}}
% \defbibnote{postnote}{\par\medskip\noindent{\footnotesize% Note:
%     \arxivp \mparcp \philscip \biorxivp}}

\printbibliography[prenote=prenote%,postnote=postnote
]

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Cut text (won't be compiled)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%% Local Variables: 
%%% mode: LaTeX
%%% TeX-PDF-mode: t
%%% TeX-master: t
%%% End: 
